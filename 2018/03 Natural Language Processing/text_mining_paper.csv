abstract,author,meta,subject,title
"The complicated, evolving landscape of cancer mutations poses a formidable
challenge to identify cancer genes among the large lists of mutations typically
generated in NGS experiments. The ability to prioritize these variants is
therefore of paramount importance. To address this issue we developed
OncoScore, a text-mining tool that ranks genes according to their association
with cancer, based on available biomedical literature. Receiver operating
characteristic curve and the area under the curve (AUC) metrics on manually
curated datasets confirmed the excellent discriminating capability of OncoScore
(OncoScore cut-off threshold = 21.09; AUC = 90.3%, 95% CI: 88.1-92.5%),
indicating that OncoScore provides useful results in cases where an efficient
prioritization of cancer-associated genes is needed.
","Rocco Piazza, Daniele Ramazzotti, Roberta Spinelli, Alessandra Pirola, Luca De Sano, Pierangelo Ferrari, Vera Magistroni, Nicoletta Cordani, Nitesh Sharma, Carlo Gambacorti-Passerini","Thu, 9 Mar 2017 01:24:23 GMT (948kb)",Genomics (q-bio.GN),"OncoScore: a novel, Internet-based tool to assess the oncogenic potential of genes"
"Mining textual patterns in news, tweets, papers, and many other kinds of text
corpora has been an active theme in text mining and NLP research. Previous
studies adopt a dependency parsing-based pattern discovery approach. However,
the parsing results lose rich context around entities in the patterns, and the
process is costly for a corpus of large scale. In this study, we propose a
novel typed textual pattern structure, called meta pattern, which is extended
to a frequent, informative, and precise subsequence pattern in certain context.
We propose an efficient framework, called MetaPAD, which discovers meta
patterns from massive corpora with three techniques: (1) it develops a
context-aware segmentation method to carefully determine the boundaries of
patterns with a learnt pattern quality assessment function, which avoids costly
dependency parsing and generates high-quality patterns; (2) it identifies and
groups synonymous meta patterns from multiple facets---their types, contexts,
and extractions; and (3) it examines type distributions of entities in the
instances extracted by each group of patterns, and looks for appropriate type
levels to make discovered patterns precise. Experiments demonstrate that our
proposed framework discovers high-quality typed textual patterns efficiently
from different genres of massive corpora and facilitates information
extraction.
","Meng Jiang, Jingbo Shang, Taylor Cassidy, Xiang Ren, Lance M. Kaplan, Timothy P. Hanratty, Jiawei Han","Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] Tue, 14 Mar 2017 20:26:32 GMT (1150kb,D)",Computation and Language (cs.CL),MetaPAD: Meta Pattern Discovery from Massive Text Corpora
"This paper is a tutorial on Formal Concept Analysis (FCA) and its
applications. FCA is an applied branch of Lattice Theory, a mathematical
discipline which enables formalisation of concepts as basic units of human
thinking and analysing data in the object-attribute form. Originated in early
80s, during the last three decades, it became a popular human-centred tool for
knowledge representation and data analysis with numerous applications. Since
the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics
include Information Retrieval with a focus on visualisation aspects, Machine
Learning, Data Mining and Knowledge Discovery, Text Mining and several others.
",Dmitry I. Ignatov,"Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)",Information Retrieval (cs.IR),Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields
"Topic models have been widely used in discovering latent topics which are
shared across documents in text mining. Vector representations, word embeddings
and topic embeddings, map words and topics into a low-dimensional and dense
real-value vector space, which have obtained high performance in NLP tasks.
However, most of the existing models assume the result trained by one of them
are perfect correct and used as prior knowledge for improving the other model.
Some other models use the information trained from external large corpus to
help improving smaller corpus. In this paper, we aim to build such an algorithm
framework that makes topic models and vector representations mutually improve
each other within the same corpus. An EM-style algorithm framework is employed
to iteratively optimize both topic model and vector representations.
Experimental results show that our model outperforms state-of-art methods on
various NLP tasks.
","Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu Rong (Dept. of Computer Science, Sun Yat-Sen University, GuangZhou, China.)","Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)",Computation and Language (cs.CL),LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations
"Entity extraction is fundamental to many text mining tasks such as
organisation name recognition. A popular approach to entity extraction is based
on matching sub-string candidates in a document against a dictionary of
entities. To handle spelling errors and name variations of entities, usually
the matching is approximate and edit or Jaccard distance is used to measure
dissimilarity between sub-string candidates and the entities. For approximate
entity extraction from free text, existing work considers solely
character-based or solely token-based similarity and hence cannot
simultaneously deal with minor variations at token level and typos. In this
paper, we address this problem by considering both character-based similarity
and token-based similarity (i.e. two-level similarity). Measuring one-level
(e.g. character-based) similarity is computationally expensive, and measuring
two-level similarity is dramatically more expensive. By exploiting the
properties of the two-level similarity and the weights of tokens, we develop
novel techniques to significantly reduce the number of sub-string candidates
that require computation of two-level similarity against the dictionary of
entities. A comprehensive experimental study on real world datasets show that
our algorithm can efficiently extract entities from documents and produce a
high F1 score in the range of [0.91, 0.97].
","Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramamohanarao","Sun, 12 Feb 2017 12:46:40 GMT (89kb)",Databases (cs.DB),A Technical Report: Entity Extraction using Both Character-based and Token-based Similarity
"Explicit concept space models have proven efficacy for text representation in
many natural language and text mining applications. The idea is to embed
textual structures into a semantic space of concepts which captures the main
topics of these structures. That so called bag-of-concepts representation
suffers from data sparsity causing low similarity scores between similar texts
due to low concept overlap. In this paper we propose two neural embedding
models in order to learn continuous concept vectors. Once learned, we propose
an efficient vector aggregation method to generate fully dense bag-of-concepts
representations. Empirical results on a benchmark dataset for measuring entity
semantic relatedness show superior performance over other concept embedding
models. In addition, by utilizing our efficient aggregation method, we
demonstrate the effectiveness of the densified vector representation over the
typical sparse representations for dataless classification where we can achieve
at least same or better accuracy with much less dimensions.
","Walid Shalaby, Wlodek Zadrozny","Fri, 10 Feb 2017 22:44:59 GMT (344kb)",Computation and Language (cs.CL),Learning Concept Embeddings for Efficient Bag-of-Concepts Densification
"Many contemporary statistical learning methods assume a Euclidean feature
space, however, the ""curse of dimensionality"" associated with high feature
dimensions is particularly severe for the Euclidean distance. This paper
presents a method for defining similarity based on hyperspherical geometry and
shows that it often improves the performance of support vector machine compared
to other competing similarity measures. Specifically, the idea of using heat
diffusion on a hypersphere to measure similarity has been proposed and tested
by \citet{Lafferty:2015uy}, demonstrating promising results based on an
approximate heat kernel, however, the exact hyperspherical heat kernel hitherto
remains unknown. In this paper, we derive an exact form of the heat kernel on a
unit hypersphere in terms of a uniformly and absolutely convergent series in
high-dimensional angular momentum eigenmodes. Being a natural measure of
similarity between sample points dwelling on a hypersphere, the exact kernel
often shows superior performance in kernel SVM classifications applied to text
mining, tumor somatic mutation imputation, and stock market analysis. The
improvement in classification accuracy compared with kernels based on Euclidean
geometry may arise from ameliorating the curse of dimensionality on compact
manifolds.
","Chenchao Zhao, Jun S. Song","Sun, 5 Feb 2017 04:55:14 GMT (1209kb,D)",Machine Learning (stat.ML),Exact heat kernel on a hypersphere and its applications in kernel SVM
"The edit distance is a basic string similarity measure used in many
applications such as text mining, signal processing, bioinformatics, and so on.
However, the computational cost can be a problem when we repeat many distance
calculations as seen in real-life searching situations. A promising solution to
cope with the problem is to approximate the edit distance by another distance
with a lower computational cost. There are, indeed, many distances have been
proposed for approximating the edit distance. However, their approximation
accuracies are evaluated only theoretically: many of them are evaluated only
with big-oh (asymptotic) notations, and without experimental analysis.
Therefore, it is beneficial to know their actual performance in real
applications. In this study we compared existing six approximation distances in
two approaches: (i) we refined their theoretical approximation accuracy by
calculating up to the constant coefficients, and (ii) we conducted some
experiments, in one artificial and two real-life data sets, to reveal under
which situations they perform best. As a result we obtained the following
results: [Batu 2006] is the best theoretically and [Andoni 2010]
experimentally. Theoretical considerations show that [Batu 2006] is the best if
the string length n is large enough (n >= 300). [Andoni 2010] is experimentally
the best for most data sets and theoretically the second best. [Bar-Yossef
2004], [Charikar 2006] and [Sokolov 2007], despite their middle-level
theoretical performance, are experimentally as good as [Andoni 2010] for pairs
of strings with large alphabet size.
","Hiroyuki Hanada, Mineichi Kudo, Atsuyoshi Nakamura","Sun, 22 Jan 2017 07:40:52 GMT (1402kb)",Data Structures and Algorithms (cs.DS),On Practical Accuracy of Edit Distance Approximation Algorithms
"With the rapid growth of social media on the web, emotional polarity
computation has become a flourishing frontier in the text mining community.
However, it is challenging to understand the latest trends and summarize the
state or general opinions about products due to the big diversity and size of
social media data and this creates the need of automated and real time opinion
extraction and mining. On the other hand, the bulk of current research has been
devoted to study the subjective sentences which contain opinion keywords and
limited work has been reported for objective statements that imply sentiment.
In this paper, fuzzy based knowledge engineering model has been developed for
sentiment classification of special group of such sentences including the
change or deviation from desired range or value. Drug reviews are the rich
source of such statements. Therefore, in this research, some experiments were
carried out on patient's reviews on several different cholesterol lowering
drugs to determine their sentiment polarity. The main conclusion through this
study is, in order to increase the accuracy level of existing drug opinion
mining systems, objective sentences which imply opinion should be taken into
account. Our experimental results demonstrate that our proposed model obtains
over 72 percent F1 value.
","Amir Hossein Yazdavar, Monireh Ebrahimi, Naomie Salim","Tue, 3 Jan 2017 19:41:24 GMT (1049kb)",Computation and Language (cs.CL),Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences
"We introduce our explorative historical leveled approach that we use to
understand drug debates in the Royal Dutch Library's digital newspaper archive.
In this approach we alternate between distant reading and close reading.
Furthermore, we use this approach to evaluate two text mining tools:
AVResearcherXL and Texcavator.
","Berrie van der Molen, Lars Buitinck, Toine Pieters","Mon, 2 Jan 2017 12:16:11 GMT (135kb)",Human-Computer Interaction (cs.HC),The leveled approach. Using and evaluating text mining tools AVResearcherXL and Texcavator for historical research on public perceptions of drugs
"This paper deals with the entity extraction task (named entity recognition)
of a text mining process that aims at unveiling non-trivial semantic
structures, such as relationships and interaction between entities or
communities. In this paper we present a simple and efficient named entity
extraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging
based algorithm for NER), relies on flexible pattern matching, part-of-speech
tagging and lexical-based rules. It was developed to process texts written in
Portuguese, however it is potentially applicable to other languages as well.
We compare our approach with current alternatives that support Named Entity
Recognition (NER) for content written in Portuguese. These are Alchemy, Zemanta
and Rembrandt. Evaluation of the efficacy of the entity extraction method on
several texts written in Portuguese indicates a considerable improvement on
$recall$ and $F_1$ measures.
","Concei??o Rocha, Al?pio Jorge, Roberta Sionara, Paula Brito, Carlos Pimenta, Solange Rezende","Fri, 30 Dec 2016 17:10:29 GMT (131kb)",Information Retrieval (cs.IR),PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese
"This paper studies marijuana-related tweets in social network Twitter. We
collected more than 300,000 marijuana related tweets during November 2016 in
our study. Our text-mining based algorithms and data analysis unveil some
interesting patterns including: (i) users' attitudes (e.g., positive or
negative) can be characterized by the existence of outer links in a tweet; (ii)
67% users use their mobile phones to post their messages while many users
publish their messages using third-party automatic posting services; and (3)
the number of tweets during weekends is much higher than during weekdays. Our
data also showed the impact of the political events such as the U.S.
presidential election or state marijuana legalization votes on the
marijuana-related tweeting frequencies.
","Anh Nguyen, Quang Hoang, Hung Nguyen, Dong Nguyen, Tuan Tran","Wed, 28 Dec 2016 16:11:28 GMT (740kb,D)",Social and Information Networks (cs.SI),Evaluating Marijuana-Related Tweets On Twitter
"Recent studies have shown that information mined from Craigslist can be used
for informing public health policy or monitoring risk behavior. This paper
presents a text-mining method for conducting public health surveillance of
marijuana use concerns in the U.S. using online classified ads in Craigslist.
We collected more than 200 thousands of rental ads in the housing categories in
Craigslist and devised text-mining methods for efficiently and accurately
extract rental ads associated with concerns about the uses of marijuana in
different states across the U.S. We linked the extracted ads to their
geographic locations and computed summary statistics of the ads having
marijuana use concerns. Our data is then compared with the State Marijuana Laws
Map published by the U.S. government and marijuana related keywords search in
Google to verify our collected data with respect to the demographics of
marijuana use concerns. Our data not only indicates strong correlations between
Craigslist ads, Google search and the State Marijuana Laws Map in states where
marijuana uses are legal, but also reveals some hidden world of marijuana use
concerns in other states where marijuana use is illegal. Our approach can be
utilized as a marijuana surveillance tool for policy makers to develop public
health policy and regulations.
","Anh Nguyen, Long Nguyen, Dong Nguyen, Uyen Le, Tuan Tran","Thu, 22 Dec 2016 15:00:16 GMT (509kb,D) [v2] Wed, 28 Dec 2016 16:13:48 GMT (509kb,D)",Computers and Society (cs.CY),"""420 Friendly"": Revealing Marijuana Use via Craigslist Rental Ads"
"A good lexicon is an important resource for various cross-lingual tasks such
as information retrieval and text mining. In this paper, we focus on extracting
translation pairs from non-parallel cross-lingual corpora. Previous lexicon
extraction algorithms for non-parallel data generally rely on an accurate seed
dictionary and extract translation pairs by using context similarity. However,
there are two problems. One, a lot of semantic information is lost if we just
use seed dictionary words to construct context vectors and obtain the context
similarity. Two, in practice, we may not have a clean seed dictionary. For
example, if we use a generic dictionary as a seed dictionary in a special
domain, it might be very noisy. To solve these two problems, we propose two new
bilingual topic models to better capture the semantic information of each word
while discriminating the multiple translations in a noisy seed dictionary. We
then use an effective measure to evaluate the similarity of words in different
languages and select the optimal translation pairs. Results of experiments
using real Japanese-English data demonstrate the effectiveness of our models.
",Tengfei Ma,"Wed, 21 Dec 2016 16:12:45 GMT (166kb,D)",Computation and Language (cs.CL),Inverted Bilingual Topic Models for Lexicon Extraction from Non-parallel Data
"Non-negative matrix factorization ( NMF ) is a new knowledge discovery method
that is used for text mining, signal processing, bioinformatics, and consumer
analysis. However, its basic property as a learning machine is not yet
clarified, as it is not a regular statistical model, resulting that theoretical
optimization method of NMF has not yet established. In this paper, we study the
real log canonical threshold of NMF and give an upper bound of the
generalization error in Bayesian learning. The results show that the
generalization error of the matrix factorization can be made smaller than
regular statistical models if Bayesian learning is applied.
","Naoki Hayashi, Sumio Watanabe","Tue, 13 Dec 2016 12:02:24 GMT (11kb) [v2] Wed, 15 Feb 2017 07:24:03 GMT (14kb) [v3] Wed, 22 Feb 2017 10:30:36 GMT (14kb)",Statistics Theory (math.ST),Upper Bound of Bayesian Generalization Error in Non-Negative Matrix Factorization
"This paper presents an algorithm for the unsupervised learning of latent
variable models from unlabeled sets of data. We base our technique on spectral
decomposition, providing a technique that proves to be robust both in theory
and in practice. We also describe how to use this algorithm to learn the
parameters of two well known text mining models: single topic model and Latent
Dirichlet Allocation, providing in both cases an efficient technique to
retrieve the parameters to feed the algorithm. We compare the results of our
algorithm with those of existing algorithms on synthetic data, and we provide
examples of applications to real world text corpora for both single topic model
and LDA, obtaining meaningful results.
","Matteo Ruffini, Marta Casanellas, Ricard Gavald?","Sun, 11 Dec 2016 13:31:58 GMT (363kb,D)",Machine Learning (stat.ML),A New Spectral Method for Latent Variable Models
"Sentiment analysis is one of the fastest growing research areas in computer
science, making it challenging to keep track of all the activities in the area.
We present a computer-assisted literature review, where we utilize both text
mining and qualitative coding, and analyze 6,996 papers from Scopus. We find
that the roots of sentiment analysis are in the studies on public opinion
analysis at the beginning of 20th century and in the text subjectivity analysis
performed by the computational linguistics community in 1990's. However, the
outbreak of computer-based sentiment analysis only occurred with the
availability of subjective texts on the Web. Consequently, 99% of the papers
have been published after 2004. Sentiment analysis papers are scattered to
multiple publication venues, and the combined number of papers in the top-15
venues only represent ca. 30% of the papers in total. We present the top-20
cited papers from Google Scholar and Scopus and a taxonomy of research topics.
In recent years, sentiment analysis has shifted from analyzing online product
reviews to social media texts from Twitter and Facebook. Many topics beyond
product reviews like stock markets, elections, disasters, medicine, software
development and cyberbullying extend the utilization of sentiment analysis
","Mika Viking M?ntyl?, Daniel Graziotin, Miikka Kuutila","Mon, 5 Dec 2016 21:28:06 GMT (1732kb) [v2] Tue, 21 Mar 2017 18:09:41 GMT (1205kb)",Computation and Language (cs.CL),"The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers"
"In this paper, we have identified and analyzed the emergence, structure and
dynamics of the paradigmatic research fronts that established the fundamentals
of the biomedical knowledge on HIV/AIDS. A search of papers with the
identifiers ""HIV/AIDS"", ""Human Immunodeficiency Virus"" and ""Acquired
Immunodeficiency Syndrome"" in the Web of Science (Thomson Reuters), was carried
out. A citation network of those papers was constructed. Then, a sub-network of
the papers with the highest number of inter-citations (with a minimal in-degree
of 30) was selected to perform a combination of network clustering and text
mining to identify the paradigmatic research fronts and analyze their dynamics.
Seven research fronts were identified in this sub-network. Each one of the
fronts are related to one of the following topics: ""preliminary pathology
research"", ""virus entry"", ""virus tropism"", ""drug resistance"", ""genetic
expression"", ""assembly"", and ""virus replication"". The emergence of these fronts
occurred in successive ""waves"" over the time which suggest a transition in the
paradigmatic focus. The emergence of the biomedical fronts in HIV/AIDS research
is explained not just by the partition of the problem in elements and
interactions leading to increasingly specialized communities, but also by
changes in the technological context of this health problem.
","David Fajardo-Ortiz, Malaquias Lopez-Cervantes, Luis Duran, Michel Dumontier, Miguel Lara, Hector Ochoa, Victor M Castano","Wed, 16 Nov 2016 10:13:08 GMT (2080kb)",Social and Information Networks (cs.SI),The structure and dynamics of the research fronts in HIV/AIDS research: part-whole and adaptive specialization
"Document similarity is the problem of formally representing textual documents
and then proposing a similarity measure that can be used to compute the
linguistic similarity between two documents. Accurate document similarity
computation improves many enterprise relevant tasks such as document
clustering, text mining, and question-answering. Most contemporary techniques
employ bag-of-words (BoW) based document representation models. In this paper,
we show that a document's thematic flow, which is often disregarded by
bag-of-word techniques, is pivotal in estimating their semantic similarity. In
this direction, we propose a novel semantic document similarity framework,
called SimDoc. We model documents as topic-sequences, where topics represent
latent generative clusters of relative words. We then use a sequence alignment
algorithm, that has been adapted from the Smith-Waterman gene-sequencing
algorithm, to estimate their semantic similarity. For similarity computation at
a finer granularity, we tune the alignment algorithm by integrating it with a
word embedding matrix based topic-to-topic similarity measure. A document level
similarity score is then computed by again using the sequence alignment
algorithm over all sentence pairs. In our experiments, we see that SimDoc
outperforms many contemporary bag-of-words techniques in accurately computing
document similarity, and on practical applications such as document clustering.
","Gaurav Maheshwari, Priyansh Trivedi, Harshita Sahijwani, Kunal Jha, Sourish Dasgupta, Jens Lehmann","Tue, 15 Nov 2016 13:31:28 GMT (323kb,D)",Computation and Language (cs.CL),SimDoc: Topic Sequence Alignment based Document Similarity Framework
"Abuse in any form is a grave threat to a child's health. Public health
institutions in the Netherlands try to identify and prevent different kinds of
abuse, and building a decision support system can help such institutions
achieve this goal. Such decision support relies on the analysis of relevant
child health data. A significant part of the medical data that the institutions
have on children is unstructured, and in the form of free text notes. In this
research, we employ machine learning and text mining techniques to detect
patterns of possible child abuse in the data. The resulting model achieves a
high score in classifying cases of possible abuse. We then describe our
implementation of the decision support API at a municipality in the
Netherlands.
","Chintan Amrit, Tim Paauw, Robin Aly, Miha Lavric","Fri, 11 Nov 2016 11:27:07 GMT (2805kb,D) [v2] Wed, 16 Nov 2016 09:57:41 GMT (2805kb,D)",Computers and Society (cs.CY),Using text mining and machine learning for detection of child abuse
"Generalized linear model with $L_1$ and $L_2$ regularization is a widely used
technique for solving classification, class probability estimation and
regression problems. With the numbers of both features and examples growing
rapidly in the fields like text mining and clickstream data analysis
parallelization and the use of cluster architectures becomes important. We
present a novel algorithm for fitting regularized generalized linear models in
the distributed environment. The algorithm splits data between nodes by
features, uses coordinate descent on each node and line search to merge results
globally. Convergence proof is provided. A modifications of the algorithm
addresses slow node problem. For an important particular case of logistic
regression we empirically compare our program with several state-of-the art
approaches that rely on different algorithmic and data spitting methods.
Experiments demonstrate that our approach is scalable and superior when
training on large and sparse datasets.
","Ilya Trofimov, Alexander Genkin","Mon, 7 Nov 2016 15:19:54 GMT (1684kb,D)",Machine Learning (stat.ML),Distributed Coordinate Descent for Generalized Linear Models with Regularization
"Blockchain represents a technology for establishing a shared, immutable
version of the truth between a network of participants that do not trust one
another, and therefore has the potential to disrupt any financial or other
industries that rely on third-parties to establish trust. Recent trends in
computing including: prevalence of Free and Open Source Software (FOSS); easy
access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly
advanced analytics capabilities such as Natural Language Processing (NLP) and
Machine Learning (ML) allow for rapidly prototyping applications for analysis
of trends in the emergence of Blockchain technology. A scaleable
proof-of-concept pipeline that lays the groundwork for analysis of multiple
streams of semi-structured data posted on social media is demonstrated.
Preliminary analysis and performance metrics are presented and discussed.
Future work is described that will scale the system to cloud-based, real-time,
analysis of multiple data streams, with Information Extraction (IE) (ex.
sentiment analysis) and Machine Learning capability.
","Marek Laskowski, Henry M. Kim","Sun, 28 Aug 2016 20:04:25 GMT (669kb)",Computers and Society (cs.CY),Rapid Prototyping of a Text Mining Application for Cryptocurrency Market Intelligence
"Assisted text input techniques can save time and effort and improve text
quality. In this paper, we investigate how grounded and conditional extensions
to standard neural language models can bring improvements in the tasks of word
prediction and completion. These extensions incorporate a structured knowledge
base and numerical values from the text into the context used to predict the
next word. Our automated evaluation on a clinical dataset shows extended models
significantly outperform standard models. Our best system uses both
conditioning and grounding, because of their orthogonal benefits. For word
prediction with a list of 5 suggestions, it improves recall from 25.03% to
71.28% and for word completion it improves keystroke savings from 34.35% to
44.81%, where theoretical bound for this dataset is 58.78%. We also perform a
qualitative investigation of how models with lower perplexity occasionally fare
better at the tasks. We found that at test time numbers have more influence on
the document level than on individual word probabilities.
","Georgios P. Spithourakis, Steffen E. Petersen, Sebastian Riedel","Thu, 20 Oct 2016 11:48:30 GMT (502kb,D)",Computation and Language (cs.CL),Clinical Text Prediction with Numerically Grounded Conditional Language Models
"One essential task in information extraction from the medical corpus is drug
name recognition. Compared with text sources come from other domains, the
medical text is special and has unique characteristics. In addition, the
medical text mining poses more challenges, e.g., more unstructured text, the
fast growing of new terms addition, a wide range of name variation for the same
drug. The mining is even more challenging due to the lack of labeled dataset
sources and external knowledge, as well as multiple token representations for a
single drug name that is more common in the real application setting. Although
many approaches have been proposed to overwhelm the task, some problems
remained with poor F-score performance (less than 0.75). This paper presents a
new treatment in data representation techniques to overcome some of those
challenges. We propose three data representation techniques based on the
characteristics of word distribution and word similarities as a result of word
embedding training. The first technique is evaluated with the standard NN
model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two
deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked
Denoising Encoders). The third technique represents the sentence as a sequence
that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term
Memory). In extracting the drug name entities, the third technique gives the
best F-score performance compared to the state of the art, with its average
F-score being 0.8645.
","Sadikin Mujiono, Mohamad Ivan Fanany, Chan Basaruddin","Thu, 6 Oct 2016 14:38:09 GMT (337kb,D)",Computation and Language (cs.CL),A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text
"Non-negative matrix factorization (NMF) is the problem of determining two
non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such
that $A \approx W H$. NMF is a useful tool for many applications in different
domains such as topic modeling in text mining, background separation in video
analysis, and community detection in social networks. Despite its popularity in
the data mining community, there is a lack of efficient parallel algorithms to
solve the problem for big data sets.
The main contribution of this work is a new, high-performance parallel
computational framework for a broad class of NMF algorithms that iteratively
solves alternating non-negative least squares (NLS) subproblems for $W$ and
$H$. It maintains the data and factor matrices in memory (distributed across
processors), uses MPI for interprocessor communication, and, in the dense case,
provably minimizes communication costs (under mild assumptions). The framework
is flexible and able to leverage a variety of NMF and NLS algorithms, including
Multiplicative Update, Hierarchical Alternating Least Squares, and Block
Principal Pivoting. Our implementation allows us to benchmark and compare
different algorithms on massive dense and sparse data matrices of size that
spans for few hundreds of millions to billions. We demonstrate the scalability
of our algorithm and compare it with baseline implementations, showing
significant performance improvements. The code and the datasets used for
conducting the experiments are available online.
","Ramakrishnan Kannan, Grey Ballard, Haesun Park","Wed, 28 Sep 2016 23:31:45 GMT (4171kb,D)","Distributed, Parallel, and Cluster Computing (cs.DC)",MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization
"Natural language processing techniques are increasingly applied to identify
social trends and predict behavior based on large text collections. Existing
methods typically rely on surface lexical and syntactic information. Yet,
research in psychology shows that patterns of human conceptualisation, such as
metaphorical framing, are reliable predictors of human expectations and
decisions. In this paper, we present a method to learn patterns of metaphorical
framing from large text collections, using statistical techniques. We apply the
method to data in three different languages and evaluate the identified
patterns, demonstrating their psychological validity.
","Ekaterina Shutova, Patricia Lichtenstein","Wed, 28 Sep 2016 17:58:23 GMT (549kb,D)",Computation and Language (cs.CL),Psychologically Motivated Text Mining
"Drug name recognition (DNR) is an essential step in the Pharmacovigilance
(PV) pipeline. DNR aims to find drug name mentions in unstructured biomedical
texts and classify them into predefined categories. State-of-the-art DNR
approaches heavily rely on hand crafted features and domain specific resources
which are difficult to collect and tune. For this reason, this paper
investigates the effectiveness of contemporary recurrent neural architectures -
the Elman and Jordan networks and the bidirectional LSTM with CRF decoding - at
performing DNR straight from the text. The experimental results achieved on the
authoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional
LSTM-CRF ranks closely to highly-dedicated, hand-crafted systems.
","Raghavendra Chalapathy, Ehsan Zare Borzeshi, Massimo Piccardi","Sat, 24 Sep 2016 08:45:17 GMT (16kb)",Computation and Language (cs.CL),An Investigation of Recurrent Neural Architectures for Drug Name Recognition
"Social Engineering (SE) is one of the most dangerous aspect an attacker can
use against a given entity (private citizen, industry, government, ...). In
order to perform SE attacks, it is necessary to collect as much information as
possible about the target (or victim(s)). The aim of this paper is to report
the details of an activity which took to the development of an automatic tool
that extracts, categorizes and summarizes the target interests, thus possible
weaknesses with respect to specific topics. Data is collected from the user's
activity on social networks, parsed and analyzed using text mining techniques.
The main contribution of the proposed tool consists in delivering some reports
that allow the citizen, institutions as well as private bodies the screening of
their exposure to SE attacks, with a strong awareness potential that will be
reflected in a decrease of the risks and a good opportunity to save money.
","Alan Ferrari, Angelo Consoli","Fri, 23 Sep 2016 10:35:28 GMT (216kb,D)",Cryptography and Security (cs.CR),Building accurate HAV exploiting User Profiling and Sentiment Analysis
"Context: Topic modeling finds human-readable structures in unstructured
textual data. A widely used topic modeler is Latent Dirichlet allocation. When
run on different datasets, LDA suffers from ""order effects"" i.e. different
topics are generated if the order of training data is shuffled. Such order
effects introduce a systematic error for any study. This error can relate to
misleading results;specifically, inaccurate topic descriptions and a reduction
in the efficacy of text mining classification results. Objective: To provide a
method in which distributions generated by LDA are more stable and can be used
for further analysis. Method: We use LDADE, a search-based software engineering
tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is
evaluated on data from a programmer information exchange site (Stackoverflow),
title and abstract text of thousands ofSoftware Engineering (SE) papers, and
software defect reports from NASA. Results were collected across different
implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different
platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using
Gibbs sampling). Results were scored via topic stability and text mining
classification accuracy. Results: In all treatments: (i) standard LDA exhibits
very large topic instability; (ii) LDADE's tunings dramatically reduce cluster
instability; (iii) LDADE also leads to improved performances for supervised as
well as unsupervised learning. Conclusion: Due to topic instability, using
standard LDA with its ""off-the-shelf"" settings should now be depreciated. Also,
in future, we should require SE papers that use LDA to test and (if needed)
mitigate LDA topic instability. Finally, LDADE is a candidate technology for
effectively and efficiently reducing that instability.
","Amritanshu Agrawal, Wei Fu, Tim Menzies","Mon, 29 Aug 2016 18:45:00 GMT (3893kb,D) [v2] Wed, 8 Feb 2017 01:19:06 GMT (4153kb,D)",Software Engineering (cs.SE),What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering)
"The ubiquitous presence of sequence data across fields such as the web,
healthcare, bioinformatics, and text mining has made sequence mining a vital
research area. However, sequence mining is particularly challenging because of
difficulty in finding (dis)similarity/distance between sequences. This is
because a distance measure between sequences is not obvious due to their
unstructuredness---arbitrary strings of arbitrary length. Feature
representations, such as n-grams, are often used but they either compromise on
extracting both short- and long-term sequence patterns or have a high
computation. We propose a new function, Sequence Graph Transform (SGT), that
extracts the short- and long-term sequence features and embeds them in a
finite-dimensional feature space. Importantly, SGT has low computation and can
extract any amount of short- to long-term patterns without any increase in the
computation, also proved theoretically in this paper. Due to this, SGT yields
superior result with significantly higher accuracy and lower computation
compared to the existing methods. We show it via several experimentation and
SGT's real world application for clustering, classification, search and
visualization as examples.
","Chitta Ranjan, Samaneh Ebrahimi, Kamran Paynabar","Thu, 11 Aug 2016 16:59:19 GMT (1852kb,D) [v2] Fri, 12 Aug 2016 14:01:03 GMT (1852kb,D) [v3] Tue, 23 Aug 2016 20:03:41 GMT (1853kb,D) [v4] Wed, 28 Sep 2016 00:20:49 GMT (679kb,D) [v5] Wed, 19 Oct 2016 05:04:59 GMT (419kb,D) [v6] Sun, 27 Nov 2016 01:43:12 GMT (435kb,D) [v7] Wed, 30 Nov 2016 06:35:26 GMT (435kb,D) [v8] Tue, 31 Jan 2017 03:50:58 GMT (434kb,D)",Machine Learning (stat.ML),Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining (Extended Version)
"Source separation, which consists in decomposing data into meaningful
structured components, is an active research topic in many areas, such as music
and image signal processing, applied physics and text mining. In this paper, we
introduce the Positive $\alpha$-stable (P$\alpha$S) distributions to model the
latent sources, which are a subclass of the stable distributions family. They
notably permit us to model random variables that are both nonnegative and
impulsive. Considering the L\'evy distribution, the only P$\alpha$S
distribution whose density is tractable, we propose a mixture model called
L\'evy Nonnegative Matrix Factorization (L\'evy NMF). This model accounts for
low-rank structures in nonnegative data that possibly has high variability or
is corrupted by very adverse noise. The model parameters are estimated in a
maximum-likelihood sense. We also derive an estimator of the sources given the
parameters, which extends the validity of the generalized Wiener filtering to
the P$\alpha$S case. Experiments on synthetic data show that L\'evy NMF
compares favorably with state-of-the art techniques in terms of robustness to
impulsive noise. The analysis of two types of realistic signals is also
considered: musical spectrograms and fluorescence spectra of chemical species.
The results highlight the potential of the L\'evy NMF model for decomposing
nonnegative data.
","Paul Magron, Roland Badeau, Antoine Liutkus","Fri, 5 Aug 2016 11:48:58 GMT (436kb) [v2] Tue, 8 Nov 2016 12:25:32 GMT (367kb)",Sound (cs.SD),L?vy NMF for robust nonnegative source separation
"Unstructured data refers to information that does not have a predefined data
model or is not organized in a pre-defined manner. Loosely speaking,
unstructured data refers to text data that is generated by humans. In
after-sales service businesses, there are two main sources of unstructured
data: customer complaints, which generally describe symptoms, and technician
comments, which outline diagnostics and treatment information. A legitimate
customer complaint can eventually be tracked to a failure or a claim. However,
there is a delay between the time of a customer complaint and the time of a
failure or a claim. A proactive strategy aimed at analyzing customer complaints
for symptoms can help service providers detect reliability problems in advance
and initiate corrective actions such as recalls. This paper introduces
essential text mining concepts in the context of reliability analysis and a
method to detect emerging reliability issues. The application of the method is
illustrated using a case study.
","Deovrat Kakde, Arin Chaudhuri","Tue, 26 Jul 2016 15:19:12 GMT (373kb)",Artificial Intelligence (cs.AI),Leveraging Unstructured Data to Detect Emerging Reliability Issues
"Statistical techniques that analyze texts, referred to as text analytics,
have departed from the use of simple word count statistics towards a new
paradigm. Text mining now hinges on a more sophisticated set of methods,
including the representations in terms of complex networks. While
well-established word-adjacency (co-occurrence) methods successfully grasp
syntactical features of written texts, they are unable to represent important
aspects of textual data, such as its topical structure, i.e. the sequence of
subjects developing at a mesoscopic level along the text. Such aspects are
often overlooked by current methodologies. In order to grasp the mesoscopic
characteristics of semantical content in written texts, we devised a network
model which is able to analyze documents in a multi-scale fashion. In the
proposed model, a limited amount of adjacent paragraphs are represented as
nodes, which are connected whenever they share a minimum semantical content. To
illustrate the capabilities of our model, we present, as a case example, a
qualitative analysis of ""Alice's Adventures in Wonderland"". We show that the
mesoscopic structure of a document, modeled as a network, reveals many semantic
traits of texts. Such an approach paves the way to a myriad of semantic-based
applications. In addition, our approach is illustrated in a machine learning
context, in which texts are classified among real texts and randomized
instances.
","Henrique F. de Arruda, Filipi N. Silva, Vanessa Q. Marinho, Diego R. Amancio, Luciano da F. Costa","Thu, 30 Jun 2016 19:47:17 GMT (846kb,D) [v2] Sat, 25 Feb 2017 00:06:48 GMT (1508kb,D)",Computation and Language (cs.CL),Representation of texts as complex networks: a mesoscopic approach
"Here a collection of 1169 abstracts, which corresponds to articles that the
Journal of Marketing Research has published from 2005 to 2014, are analysed
under a novel approach. We apply several statistical methods, such as Principal
Components Analysis and Correspondence Analysis to identify the way Marketing
vocabulary is evolving. Similarly those articles that introduce new vocabulary
are identified and the preferred words by authors are also detected. In order
to provide an easy-to-understand explanation, we provide our results
graphically. A word-cloud with the most frequent words is given first. Secondly
abstracts-words are represented on the factorial plane. Finally one
representation of word-years allows us to detect changes on the vocabulary
through the passing of time.
","Igor Barahona, Daria Micaela Hernandez, Hector Hugo Perez-Villarreal","Fri, 5 Feb 2016 18:06:26 GMT (368kb)",Digital Libraries (cs.DL),How marketing vocabulary was evolving from 2005 to 2014? An illustrative application of statistical methods on text mining
"A broad bibliographical study suggests a scarcity of quantitative models of
simulation integrating both network and urban growth. This absence may be due
to diverging interests of concerned disciplines, resulting in a lack of
communication. We propose to proceed to an algorithmic systematic review to
give quantitative elements of answer to this question. A formal iterative
algorithm to retrieve corpuses of references from initial keywords, based on
text-mining, is developed and implemented. We study its convergence properties
and do a sensitivity analysis. We then apply it on queries representative of
the specific question, for which results tend to confirm the assumption of
disciplines compartmentalisation.
",Juste Raimbault,"Sat, 28 May 2016 12:55:49 GMT (576kb)",Digital Libraries (cs.DL),Models Coupling Urban Growth and Transportation Network Growth : An Algorithmic Systematic Review Approach
"There have been multiple attempts to resolve various inflection matching
problems in information retrieval. Stemming is a common approach to this end.
Among many techniques for stemming, statistical stemming has been shown to be
effective in a number of languages, particularly highly inflected languages. In
this paper we propose a method for finding affixes in different positions of a
word. Common statistical techniques heavily rely on string similarity in terms
of prefix and suffix matching. Since infixes are common in irregular/informal
inflections in morphologically complex texts, it is required to find infixes
for stemming. In this paper we propose a method whose aim is to find
statistical inflectional rules based on minimum edit distance table of word
pairs and the likelihoods of the rules in a language. These rules are used to
statistically stem words and can be used in different text mining tasks.
Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks
indicate that the proposed method significantly outperforms all the baselines
in terms of MAP.
","Javid Dadashkarimi, Hossein Nasr Esfahani, Heshaam Faili, Azadeh Shakery","Wed, 25 May 2016 12:25:26 GMT (1026kb,D) [v2] Mon, 20 Jun 2016 21:37:19 GMT (1019kb,D)",Information Retrieval (cs.IR),SS4MCT: A Statistical Stemmer for Morphologically Complex Texts
"Similes are natural language expressions used to compare unlikely things,
where the comparison is not taken literally. They are often used in everyday
communication and are an important part of cultural heritage. Having an
up-to-date corpus of similes is challenging, as they are constantly coined
and/or adapted to the contemporary times. In this paper we present a
methodology for semi-automated collection of similes from the world wide web
using text mining techniques. We expanded an existing corpus of traditional
similes (containing 333 similes) by collecting 446 additional expressions. We,
also, explore how crowdsourcing can be used to extract and curate new similes.
","Nikola Milosevic, Goran Nenadic","Fri, 20 May 2016 12:20:27 GMT (83kb,D)",Computation and Language (cs.CL),As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian
"Large-scale automated meta-analysis of neuroimaging data has recently
established itself as an important tool in advancing our understanding of human
brain function. This research has been pioneered by NeuroSynth, a database
collecting both brain activation coordinates and associated text across a large
cohort of neuroimaging research papers. One of the fundamental aspects of such
meta-analysis is text-mining. To date, word counts and more sophisticated
methods such as Latent Dirichlet Allocation have been proposed. In this work we
present an unsupervised study of the NeuroSynth text corpus using Deep
Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the
aforementioned methods, principal among which is the fact that it yields both
word and document embeddings in a high-dimensional vector space. Such
embeddings serve to facilitate the use of traditional machine learning
techniques on the text corpus. The proposed DBM model is shown to learn
embeddings with a clear semantic structure.
","Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana","Sun, 1 May 2016 09:01:13 GMT (1296kb,D)",Learning (cs.LG),Text-mining the NeuroSynth corpus using Deep Boltzmann Machines
"This research maps the knowledge translation process for two different types
of nanotechnologies applied to cancer: liposomes and metallic nanostructures
(MNs). We performed a structural analysis of citation networks and text mining
supported in controlled vocabularies. In the case of liposomes, our results
identify subnetworks (invisible colleges) associated with different therapeutic
strategies: nanopharmacology, hyperthermia, and gene therapy. Only in the
pharmacological strategy was an organized knowledge translation process
identified, which, however, is monopolized by the liposomal doxorubicins. In
the case of MNs, subnetworks are not differentiated by the type of therapeutic
strategy, and the content of the documents is still basic research. Research on
MNs is highly focused on developing a combination of molecular imaging and
photothermal therapy.
","David Fajardo-Ortiz, Luis Duran, Laura Moreno, Hector Ochoa, Victor-M Castano","Tue, 12 Apr 2016 15:06:02 GMT (1993kb)",Digital Libraries (cs.DL),Liposomes versus metallic nanostructures: differences in the process of knowledge translation in cancer
"Earlier techniques of text mining included algorithms like k-means, Naive
Bayes, SVM which classify and cluster the text document for mining relevant
information about the documents. The need for improving the mining techniques
has us searching for techniques using the available algorithms. This paper
proposes one technique which uses the auxiliary information that is present
inside the text documents to improve the mining. This auxiliary information can
be a description to the content. This information can be either useful or
completely useless for mining. The user should assess the worth of the
auxiliary information before considering this technique for text mining. In
this paper, a combination of classical clustering algorithms is used to mine
the datasets. The algorithm runs in two stages which carry out mining at
different levels of abstraction. The clustered documents would then be
classified based on the necessary groups. The proposed technique is aimed at
improved results of document clustering.
","Jinju Joby, Jyothi Korra","Fri, 15 Apr 2016 16:27:38 GMT (891kb,D)",Information Retrieval (cs.IR),Accessing accurate documents by mining auxiliary document information
"An increasing number of people are using online social networking services
(SNSs), and a significant amount of information related to experiences in
consumption is shared in this new media form. Text mining is an emerging
technique for mining useful information from the web. We aim at discovering in
particular tweets semantic patterns in consumers' discussions on social media.
Specifically, the purposes of this study are twofold: 1) finding similarity and
dissimilarity between two sets of textual documents that include consumers'
sentiment polarities, two forms of positive vs. negative opinions and 2)
driving actual content from the textual data that has a semantic trend. The
considered tweets include consumers opinions on US retail companies (e.g.,
Amazon, Walmart). Cosine similarity and K-means clustering methods are used to
achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic
modeling algorithm, is used for the latter purpose. This is the first study
which discover semantic properties of textual data in consumption context
beyond sentiment analysis. In addition to major findings, we apply LDA (Latent
Dirichlet Allocations) to the same data and drew latent topics that represent
consumers' positive opinions and negative opinions on social media.
","Eun Hee Ko, Diego Klabjan","Thu, 24 Mar 2016 15:22:52 GMT (623kb)",Computation and Language (cs.CL),Semantic Properties of Customer Sentiment in Tweets
"Samtla (Search And Mining Tools with Linguistic Analysis) is a digital
humanities system designed in collaboration with historians and linguists to
assist them with their research work in quantifying the content of any textual
corpora through approximate phrase search and document comparison. The
retrieval engine uses a character-based n-gram language model rather than the
conventional word-based one so as to achieve great flexibility in language
agnostic query processing.
The index is implemented as a space-optimised character-based suffix tree
with an accompanying database of document content and metadata. A number of
text mining tools are integrated into the system to allow researchers to
discover textual patterns, perform comparative analysis, and find out what is
currently popular in the research community.
Herein we describe the system architecture, user interface, models and
algorithms, and data storage of the Samtla system. We also present several case
studies of its usage in practice together with an evaluation of the systems'
ranking performance through crowdsourcing.
","Martyn Harris, Mark Levene, Dell Zhang, Dan Levene","Wed, 23 Mar 2016 12:02:12 GMT (5388kb,D)",Digital Libraries (cs.DL),The Anatomy of a Search and Mining System for Digital Archives
"Similar to other industries, the software engineering domain is plagued by
psychological diseases such as burnout, which lead developers to lose interest,
exhibit lower activity and/or feel powerless. Prevention is essential for such
diseases, which in turn requires early identification of symptoms. The
emotional dimensions of Valence, Arousal and Dominance (VAD) are able to derive
a person's interest (attraction), level of activation and perceived level of
control for a particular situation from textual communication, such as emails.
As an initial step towards identifying symptoms of productivity loss in
software engineering, this paper explores the VAD metrics and their properties
on 700,000 Jira issue reports containing over 2,000,000 comments, since issue
reports keep track of a developer's progress on addressing bugs or new
features. Using a general-purpose lexicon of 14,000 English words with known
VAD scores, our results show that issue reports of different type (e.g.,
Feature Request vs. Bug) have a fair variation of Valence, while increase in
issue priority (e.g., from Minor to Critical) typically increases Arousal.
Furthermore, we show that as an issue's resolution time increases, so does the
arousal of the individual the issue is assigned to. Finally, the resolution of
an issue increases valence, especially for the issue Reporter and for quickly
addressed issues. The existence of such relations between VAD and issue report
activities shows promise that text mining in the future could offer an
alternative way for work health assessment surveys.
","Mika M?ntyl?, Bram Adams, Giuseppe Destefanis, Daniel Graziotin, Marco Ortu","Mon, 14 Mar 2016 14:50:39 GMT (361kb,D)",Software Engineering (cs.SE),"Mining Valence, Arousal, and Dominance - Possibilities for Detecting Burnout and Productivity?"
"Intrusion Detection is one of major threats for organization. The approach of
intrusion detection using text processing has been one of research interests
which is gaining significant importance from researchers. In text mining based
approach for intrusion detection, system calls serve as source for mining and
predicting possibility of intrusion or attack. When an application runs, there
might be several system calls which are initiated in the background. These
system calls form the strong basis and the deciding factor for intrusion
detection. In this paper, we mainly discuss the approach for intrusion
detection by designing a distance measure which is designed by taking into
consideration the conventional Gaussian function and modified to suit the need
for similarity function. A Framework for intrusion detection is also discussed
as part of this research.
","Gunupudi RajeshKumar, N Mangathayaru, G Narsimha","Sat, 12 Mar 2016 01:12:08 GMT (994kb)",Cryptography and Security (cs.CR),Intrusion Detection A Text Mining Based Approach
"When looking at the structure of natural language, ""phrases"" and ""words"" are
central notions. We consider the problem of identifying such ""meaningful
subparts"" of language of any length and underlying composition principles in a
completely corpus-based and language-independent way without using any kind of
prior linguistic knowledge. Unsupervised methods for identifying ""phrases"",
mining subphrase structure and finding words in a fully automated way are
described. This can be considered as a step towards automatically computing a
""general dictionary and grammar of the corpus"". We hope that in the long run
variants of our approach turn out to be useful for other kind of sequence data
as well, such as, e.g., speech, genom sequences, or music annotation. Even if
we are not primarily interested in immediate applications, results obtained for
a variety of languages show that our methods are interesting for many practical
tasks in text mining, terminology extraction and lexicography, search engine
technology, and related fields.
","Stefan Gerdjikov, Klaus U. Schulz","Thu, 18 Feb 2016 12:08:05 GMT (679kb,D)",Computation and Language (cs.CL),Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure
"The information age has brought a deluge of data. Much of this is in text
form, insurmountable in scope for humans and incomprehensible in structure for
computers. Text mining is an expanding field of research that seeks to utilize
the information contained in vast document collections. General data mining
methods based on machine learning face challenges with the scale of text data,
posing a need for scalable text mining methods.
This thesis proposes a solution to scalable text mining: generative models
combined with sparse computation. A unifying formalization for generative text
models is defined, bringing together research traditions that have used
formally equivalent models, but ignored parallel developments. This framework
allows the use of methods developed in different processing tasks such as
retrieval and classification, yielding effective solutions across different
text mining tasks. Sparse computation using inverted indices is proposed for
inference on probabilistic models. This reduces the computational complexity of
the common text mining operations according to sparsity, yielding probabilistic
models with the scalability of modern search engines.
The proposed combination provides sparse generative models: a solution for
text mining that is general, effective, and scalable. Extensive experimentation
on text classification and ranked retrieval datasets are conducted, showing
that the proposed solution matches or outperforms the leading task-specific
methods in effectiveness, with a order of magnitude decrease in classification
times for Wikipedia article categorization with a million classes. The
developed methods were further applied in two 2014 Kaggle data mining prize
competitions with over a hundred competing teams, earning first and second
places.
",Antti Puurula,"Sun, 7 Feb 2016 02:49:27 GMT (4651kb,D)",Information Retrieval (cs.IR),Scalable Text Mining with Sparse Generative Models
"People are producing more written material then anytime in the history. The
increase is so high that professionals from the various fields are no more able
to cope with this amount of publications. Text mining tools can offer tools to
help them and one of the tools that can aid information retrieval and
information extraction is semantic text annotation. In this report we present
Marvin, a text annotator written in Java, which can be used as a command line
tool and as a Java library. Marvin is able to annotate text using multiple
sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.
",Nikola Milosevic,"Mon, 1 Feb 2016 13:27:34 GMT (217kb) [v2] Tue, 2 Feb 2016 11:31:17 GMT (218kb)",Artificial Intelligence (cs.AI),Marvin: Semantic annotation using multiple knowledge sources
"In Multiple Instance Learning (MIL) problem for sequence data, the learning
data consist of a set of bags where each bag contains a set of
instances/sequences. In many real world applications such as bioinformatics,
web mining, and text mining, comparing a random couple of sequences makes no
sense. In fact, each instance of each bag may have structural and/or temporal
relation with other instances in other bags. Thus, the classification task
should take into account the relation between semantically related instances
across bags. In this paper, we present two novel MIL approaches for sequence
data classification: (1) ABClass and (2) ABSim. In ABClass, each sequence is
represented by one vector of attributes. For each sequence of the unknown bag,
a discriminative classifier is applied in order to compute a partial
classification result. Then, an aggregation method is applied to these partial
results in order to generate the final result. In ABSim, we use a similarity
measure between each sequence of the unknown bag and the corresponding
sequences in the learning bags. An unknown bag is labeled with the bag that
presents more similar sequences. We applied both approaches to the problem of
bacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated and
discussed the proposed approaches on well known Ionizing Radiation Resistance
Bacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented by
primary structure of basal DNA repair proteins. The experimental results show
that both ABClass and ABSim approaches are efficient.
","Manel Zoghlami, Sabeur Aridhi, Haitham Sghaier, Mondher Maddouri, Engelbert Mephu Nguifo","Sat, 30 Jan 2016 21:15:10 GMT (302kb,D)",Learning (cs.LG),A multiple instance learning approach for sequence data with across bag dependencies
"Many brokers have adapted their operation to exploit the potential of the
web. Despite the importance of the real estate classifieds, there has been
little work in analyzing such data. In this paper we propose a two-stage
regression model that exploits the textual data in real estate classifieds. We
show how our model can be used to predict the price of a real estate
classified. We also show how our model can be used to highlight keywords that
affect the price positively or negatively. To assess our contributions, we
analyze four real world data sets, which we gathered from three different
property websites. The analysis shows that our model (which exploits textual
features) achieves significantly lower root mean squared error across the
different data sets and against variety of regression models.
",Sherief Abdallah,"Sun, 15 Nov 2015 09:00:56 GMT (121kb,D)",Information Retrieval (cs.IR),Using Text Mining To Analyze Real Estate Classifieds
"In dynamic topic modeling, the proportional contribution of a topic to a
document depends on the temporal dynamics of that topic's overall prevalence in
the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by
explicitly modeling document level topic proportions with covariates and
dynamic structure that includes polynomial trends and periodicity. A Markov
Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation
is developed for posterior inference. Conditional independencies in the model
and sampling are made explicit, and our MCMC algorithm is parallelized where
possible to allow for inference in large corpora. To address computational
bottlenecks associated with Polya-Gamma sampling, we appeal to the Central
Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random
variable. This approximation is fast and reliable for parameter values relevant
in the text mining domain. Our model and inference algorithm are validated with
multiple simulation examples, and we consider the application of modeling
trends in PubMed abstracts. We demonstrate that sharing information across
documents is critical for accurately estimating document-specific topic
proportions. We also show that explicitly modeling polynomial and periodic
behavior improves our ability to predict topic prevalence at future time
points.
","Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard","Thu, 12 Nov 2015 16:26:13 GMT (1067kb,D)",Machine Learning (stat.ML),Bayesian Analysis of Dynamic Linear Topic Models
"The growing incidents of counterfeiting and associated economic and health
consequences necessitate the development of active surveillance systems capable
of producing timely and reliable information for all stake holders in the
anti-counterfeiting fight. User generated content from social media platforms
can provide early clues about product allergies, adverse events and product
counterfeiting. This paper reports a work in progresswith contributions
including: the development of a framework for gathering and analyzing the views
and experiences of users of drug and cosmetic products using machine learning,
text mining and sentiment analysis, the application of the proposed framework
on Facebook comments and data from Twitter for brand analysis, and the
description of how to develop a product safety lexicon and training data for
modeling a machine learning classifier for drug and cosmetic product sentiment
prediction. The initial brand and product comparison results signify the
usefulness of text mining and sentiment analysis on social media data while the
use of machine learning classifier for predicting the sentiment orientation
provides a useful tool for users, product manufacturers, regulatory and
enforcement agencies to monitor brand or product sentiment trends in order to
act in the event of sudden or significant rise in negative sentiment.
","Haruna Isah, Daniel Neagu, Paul Trundle","Sun, 18 Oct 2015 20:05:08 GMT (2852kb)",Social and Information Networks (cs.SI),Social Media Analysis for Product Safety using Text Mining and Sentiment Analysis
"Content based Document Classification is one of the biggest challenges in the
context of free text mining. Current algorithms on document classifications
mostly rely on cluster analysis based on bag-of-words approach. However that
method is still being applied to many modern scientific dilemmas. It has
established a strong presence in fields like economics and social science to
merit serious attention from the researchers. In this paper we would like to
propose and explore an alternative grounded more securely on the dictionary
classification and correlatedness of words and phrases. It is expected that
application of our existing knowledge about the underlying classification
structure may lead to improvement of the classifier's performance.
","Koushiki Sarkar, Ritwika Law","Sun, 4 Oct 2015 09:24:27 GMT (774kb) [v2] Sat, 12 Dec 2015 17:56:52 GMT (781kb)",Information Retrieval (cs.IR),A Novel Approach to Document Classification using WordNet
"Non-negative matrix factorization (NMF) is the problem of determining two
non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such
that $A \approx W H$. NMF is a useful tool for many applications in different
domains such as topic modeling in text mining, background separation in video
analysis, and community detection in social networks. Despite its popularity in
the data mining community, there is a lack of efficient parallel software to
solve the problem for big datasets. Existing distributed-memory algorithms are
limited in terms of performance and applicability, as they are implemented
using Hadoop and are designed only for sparse matrices.
We propose a distributed-memory parallel algorithm that computes the
factorization by iteratively solving alternating non-negative least squares
(NLS) subproblems for $W$ and $H$. To our knowledge, our algorithm is the first
high-performance parallel algorithm for NMF. It maintains the data and factor
matrices in memory (distributed across processors), uses MPI for interprocessor
communication, and, in the dense case, provably minimizes communication costs
(under mild assumptions). As opposed to previous implementations, our algorithm
is also flexible: (1) it performs well for dense and sparse matrices, and (2)
it allows the user to choose from among multiple algorithms for solving local
NLS subproblems within the alternating iterations. We demonstrate the
scalability of our algorithm and compare it with baseline implementations,
showing significant performance improvements.
","Ramakrishnan Kannan, Grey Ballard, Haesun Park","Wed, 30 Sep 2015 19:47:39 GMT (468kb,D)","Distributed, Parallel, and Cluster Computing (cs.DC)",A High-Performance Parallel Algorithm for Nonnegative Matrix Factorization
"Text mining can be applied to many fields. One of the application is using
text mining in digital newspaper to do politic sentiment analysis. In this
paper sentiment analysis is applied to get information from digital news
articles about its positive or negative sentiment regarding particular
politician. This paper suggests a simple model to analyze digital newspaper
sentiment polarity using naive Bayes classifier method. The model uses a set of
initial data to begin with which will be updated when new information appears.
The model showed promising result when tested and can be implemented to some
other sentiment analysis problems.
","Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra","Fri, 21 Aug 2015 01:40:54 GMT (592kb)",Computation and Language (cs.CL),Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method
"Using the spread of the Ice Bucket Challenge on Twitter as a case study, this
research compared the concurrent diffusion patterns of both information and
behaviors in online social networks. Individual behaviors in taking the
Challenge were detected by applying text mining techniques to millions of
tweets. After comparing diffusion dynamics of information and behaviors at the
network level, the individual level, and the dyadic level, our analysis
revealed interesting differences and interactions between the two diffusion
processes and laid foundations for future predictive and prescriptive analytics
for information and behavior diffusion in social networks.
","Shiyao Wang, Tianbo Yang, Qi Zhang, Kang Zhao","Tue, 18 Aug 2015 19:33:58 GMT (537kb)",Social and Information Networks (cs.SI),Concurrent diffusion of information and behaviors in online social networks -- a case study of the Ice Bucket Challenge
"Decision analytics commonly focuses on the text mining of financial news
sources in order to provide managerial decision support and to predict stock
market movements. Existing predictive frameworks almost exclusively apply
traditional machine learning methods, whereas recent research indicates that
traditional machine learning methods are not sufficiently capable of extracting
suitable features and capturing the non-linear nature of complex tasks. As a
remedy, novel deep learning models aim to overcome this issue by extending
traditional neural network models with additional hidden layers. Indeed, deep
learning has been shown to outperform traditional methods in terms of
predictive performance. In this paper, we adapt the novel deep learning
technique to financial decision support. In this instance, we aim to predict
the direction of stock movements following financial disclosures. As a result,
we show how deep learning can outperform the accuracy of random forests as a
benchmark for machine learning by 5.66%.
","Ralph Fehrer, Stefan Feuerriegel","Sun, 9 Aug 2015 07:39:24 GMT (626kb,D)",Machine Learning (stat.ML),Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures
"To date, there have been massive Semi-Structured Documents (SSDs) during the
evolution of the Internet. These SSDs contain both unstructured features (e.g.,
plain text) and metadata (e.g., tags). Most previous works focused on modeling
the unstructured text, and recently, some other methods have been proposed to
model the unstructured text with specific tags. To build a general model for
SSDs remains an important problem in terms of both model fitness and
efficiency. We propose a novel method to model the SSDs by a so-called
Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the
tags and words information, not only to learn the document-topic and topic-word
distributions, but also to infer the tag-topic distributions for text mining
tasks. We present an efficient variational inference method with an EM
algorithm for estimating the model parameters. Meanwhile, we propose three
large-scale solutions for our model under the MapReduce distributed computing
platform for modeling large-scale SSDs. The experimental results show the
effectiveness, efficiency and the robustness by comparing our model with the
state-of-the-art methods in document modeling, tags prediction and text
classification. We also show the performance of the three distributed solutions
in terms of time and accuracy on document modeling.
","Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, Rong Pan","Thu, 30 Jul 2015 06:44:37 GMT (654kb)",Computation and Language (cs.CL),Tag-Weighted Topic Model For Large-scale Semi-Structured Documents
"Text mining is a process of extracting information of interest from text.
Such a method includes techniques from various areas such as Information
Retrieval (IR), Natural Language Processing (NLP), and Information Extraction
(IE). In this study, text mining methods are applied to extract causal
relations from maritime accident investigation reports collected from the
Marine Accident Investigation Branch (MAIB). These causal relations provide
information on various mechanisms behind accidents, including human and
organizational factors relating to the accident. The objective of this study is
to facilitate the analysis of the maritime accident investigation reports, by
means of extracting contributory causes with more feasibility. A careful
investigation of contributory causes from the reports provide opportunity to
improve safety in future.
Two methods have been employed in this study to extract the causal relations.
They are 1) Pattern classification method and 2) Connectives method. The
earlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers.
The latter simply searches for the words connecting cause and effect in
sentences.
The causal patterns extracted using these two methods are compared to the
manual (human expert) extraction. The pattern classification method showed a
fair and sensible performance with F-measure(average) = 65% when compared to
connectives method with F-measure(average) = 58%. This study is an evidence,
that text mining methods could be employed in extracting causal relations from
marine accident investigation reports.
",Santosh Tirunagari,"Thu, 9 Jul 2015 10:20:52 GMT (983kb,D)",Information Retrieval (cs.IR),Data Mining of Causal Relations from Text: Analysing Maritime Accident Investigation Reports
"This article is an extended version of a paper presented in the WSOM'2012
conference [1]. We display a combination of factorial projections, SOM
algorithm and graph techniques applied to a text mining problem. The corpus
contains 8 medieval manuscripts which were used to teach arithmetic techniques
to merchants. Among the techniques for Data Analysis, those used for
Lexicometry (such as Factorial Analysis) highlight the discrepancies between
manuscripts. The reason for this is that they focus on the deviation from the
independence between words and manuscripts. Still, we also want to discover and
characterize the common vocabulary among the whole corpus. Using the properties
of stochastic Kohonen maps, which define neighborhood between inputs in a
non-deterministic way, we highlight the words which seem to play a special role
in the vocabulary. We call them fickle and use them to improve both Kohonen map
robustness and significance of FCA visualization. Finally we use graph
algorithmic to exploit this fickleness for classification of words.
","Nicolas Bourgeois (SAMM), Marie Cottrell (SAMM), Benjamin D?ruelle (LAMOP), St?phane Lamass? (LAMOP), Patrick Letr?my (SAMM)","Thu, 25 Jun 2015 12:56:23 GMT (38kb)",Statistics Theory (math.ST),How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining
"The majority of big data is unstructured and of this majority the largest
chunk is text. While data mining techniques are well developed and standardized
for structured, numerical data, the realm of unstructured data is still largely
unexplored. The general focus lies on information extraction, which attempts to
retrieve known information from text. The Holy Grail, however is knowledge
discovery, where machines are expected to unearth entirely new facts and
relations that were not previously known by any human expert. Indeed,
understanding the meaning of text is often considered as one of the main
characteristics of human intelligence. The ultimate goal of semantic artificial
intelligence is to devise software that can understand the meaning of free
text, at least in the practical sense of providing new, actionable information
condensed out of a body of documents. As a stepping stone on the road to this
vision I will introduce a totally new approach to drug research, namely that of
identifying relevant information by employing a self-organizing semantic engine
to text mine large repositories of biomedical research papers, a technique
pioneered by Merck with the InfoCodex software. I will describe the methodology
and a first successful experiment for the discovery of new biomarkers and
phenotypes for diabetes and obesity on the basis of PubMed abstracts, public
clinical trials and Merck internal documents. The reported approach shows much
promise and has potential to impact fundamentally pharmaceutical research as a
way to shorten time-to-market of novel drugs, and for early recognition of dead
ends.
",Carlo A. Trugenberger,"Tue, 23 Jun 2015 18:04:14 GMT (351kb)",Artificial Intelligence (cs.AI),Scientific Discovery by Machine Intelligence: A New Avenue for Drug Research
"This paper proposes a decision support system to aid movie investment
decisions at the early stage of movie productions. The system predicts the
success of a movie based on its profitability by leveraging historical data
from various sources. Using social network analysis and text mining techniques,
the system automatically extracts several groups of features, including ""who""
are on the cast, ""what"" a movie is about, ""when"" a movie will be released, as
well as ""hybrid"" features that match ""who"" with ""what"", and ""when"" with ""what"".
Experiment results with movies during an 11-year period showed that the system
outperforms benchmark methods by a large margin in predicting movie
profitability. Novel features we proposed also made great contributions to the
prediction. In addition to designing a decision support system with practical
utilities, our analysis of key factors for movie profitability may also have
implications for theoretical research on team performance and the success of
creative work.
","Michael T. Lash, Kang Zhao","Wed, 17 Jun 2015 16:40:48 GMT (397kb,D) [v2] Fri, 29 Jan 2016 20:10:52 GMT (1109kb,D)",Artificial Intelligence (cs.AI),"Early Predictions of Movie Success: the Who, What, and When of Profitability"
"Important data are locked in ancient literature. It would be uneconomic to
produce these data again and today or to extract them without the help of text
mining technologies. Vespa is a text mining project whose aim is to extract
data on pest and crops interactions, to model and predict attacks on crops, and
to reduce the use of pesticides. A few attempts proposed an agricultural
information access. Another originality of our work is to parse documents with
a dependency of the document architecture.
","Nicolas Turenne, Mathieu Andro, Roselyne Corbi?re, Tien T. Phan","Thu, 23 Apr 2015 08:27:29 GMT (240kb)",Information Retrieval (cs.IR),Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining
"The paper proposes a text-mining based analytical framework aiming at the
cognitive organization of complex scientific discourses. The approach is based
on models recently developed in science mapping, being a generalization of the
so-called Science Overlay Mapping methodology, referred to as Topic Overlay
Mapping (TOM). It is shown that via applications of TOM in visualization,
document clustering, time series analysis etc. the in-depth exploration and
even the measurement of cognitive complexity and its dynamics is feasible for
scientific domains. As a use case, an empirical study is presented into the
discovery of a long-standing complex, interdisciplinary discourse, the debate
on the species concept in biosystematics.
","Sandor Soos (Dept. Science Policy and Scientometrics, Library and Information Centre of the Hungarian Academy of Sciences, MTA)","Wed, 22 Apr 2015 14:17:38 GMT (1383kb)",Digital Libraries (cs.DL),A new generation of science overlay maps with an application to the history of biosystematics
"We present results of expanding the contents of the China Biographical
Database by text mining historical local gazetteers, difangzhi. The goal of the
database is to see how people are connected together, through kinship, social
connections, and the places and offices in which they served. The gazetteers
are the single most important collection of names and offices covering the Song
through Qing periods. Although we begin with local officials we shall
eventually include lists of local examination candidates, people from the
locality who served in government, and notable local figures with biographies.
The more data we collect the more connections emerge. The value of doing
systematic text mining work is that we can identify relevant connections that
are either directly informative or can become useful without deep historical
research. Academia Sinica is developing a name database for officials in the
central governments of the Ming and Qing dynasties.
","Peter K. Bol, Chao-Lin Liu, Hongsu Wang","Wed, 8 Apr 2015 22:38:35 GMT (969kb)",Computation and Language (cs.CL),Mining and discovering biographical information in Difangzhi with a language-model-based approach
"Incorporating the side information of text corpus, i.e., authors, time
stamps, and emotional tags, into the traditional text mining models has gained
significant interests in the area of information retrieval, statistical natural
language processing, and machine learning. One branch of these works is the
so-called Author Topic Model (ATM), which incorporates the authors's interests
as side information into the classical topic model. However, the existing ATM
needs to predefine the number of topics, which is difficult and inappropriate
in many real-world settings. In this paper, we propose an Infinite Author Topic
(IAT) model to resolve this issue. Instead of assigning a discrete probability
on fixed number of topics, we use a stochastic process to determine the number
of topics from the data itself. To be specific, we extend a gamma-negative
binomial process to three levels in order to capture the
author-document-keyword hierarchical structure. Furthermore, each document is
assigned a mixed gamma process that accounts for the multi-author's
contribution towards this document. An efficient Gibbs sampling inference
algorithm with each conditional distribution being closed-form is developed for
the IAT model. Experiments on several real-world datasets show the capabilities
of our IAT model to learn the hidden topics, authors' interests on these topics
and the number of topics simultaneously.
","Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo","Mon, 30 Mar 2015 05:03:37 GMT (472kb)",Machine Learning (stat.ML),Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process
"Searching through a large volume of data is very critical for companies,
scientists, and searching engines applications due to time complexity and
memory complexity. In this paper, a new technique of generating FuzzyFind
Dictionary for text mining was introduced. We simply mapped the 23 bits of the
English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more
FuzzyFind Dictionary, and reflecting the presence or absence of particular
letters. This representation preserves closeness of word distortions in terms
of closeness of the created binary vectors within Hamming distance of 2
deviations. This paper talks about the Golay Coding Transformation Hash Table
and how it can be used on a FuzzyFind Dictionary as a new technology for using
in searching through big data. This method is introduced by linear time
complexity for generating the dictionary and constant time complexity to access
the data and update by new data sets, also updating for new data sets is linear
time depends on new data points. This technique is based on searching only for
letters of English that each segment has 23 bits, and also we have more than
23-bit and also it could work with more segments as reference table.
","Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby, Simon Y. Berkovich","Sun, 22 Mar 2015 21:46:12 GMT (1121kb)",Databases (cs.DB),Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications
"Text Clustering is a text mining technique which divides the given set of
text documents into significant clusters. It is used for organizing a huge
number of text documents into a well-organized form. In the majority of the
clustering algorithms, the number of clusters must be specified apriori, which
is a drawback of these algorithms. The aim of this paper is to show
experimentally how to determine the number of clusters based on cluster
quality. Since partitional clustering algorithms are well-suited for clustering
large document datasets, we have confined our analysis to a partitional
clustering algorithm.
","G. Hannah Grace, Kalyani Desikan","Tue, 10 Mar 2015 10:34:06 GMT (465kb)",Information Retrieval (cs.IR),Experimental Estimation of Number of Clusters Based on Cluster Quality
"In this paper, we study idea mining from crowdsourcing applications which
encourage a group of people, who are usually undefined and very large sized, to
generate ideas for new product development (NPD). In order to isolate the
relatively small number of potential ones among ideas from crowd, decision
makers not only have to identify the key textual information representing the
ideas, but they also need to consider online opinions of people who gave
comments and votes on the ideas. Due to the extremely large size of text data
generated by people on the Internet, identifying textual information has been
carried out in manual ways, and has been considered very time consuming and
costly. To overcome the ineffectiveness, this paper introduces a novel
framework that can help decision makers discover ideas having the potential to
be used in an NPD process. To achieve this, a semi-automatic text mining
technique that retrieves useful text patterns from ideas posted on
crowdsourcing application is proposed. Then, we provide an online learning
algorithm to evaluate whether the idea is potential or not. Finally to verify
the effectiveness of our algorithm, we conducted experiments on the data, which
are collected from an existing crowd sourcing website.
","Thanh-Cong Dinh, Hyerim Bae, Jaehun Park, Joonsoo Bae","Wed, 25 Feb 2015 00:13:44 GMT (2279kb)",Information Retrieval (cs.IR),A framework to discover potential ideas of new product development from crowdsourcing application
"One of the most significant problems which inhibits further developments in
the areas of Knowledge Representation and Artificial Intelligence is a problem
of semantic alignment or knowledge mapping. The progress in its solution will
be greatly beneficial for further advances of information retrieval, ontology
alignment, relevance calculation, text mining, natural language processing etc.
In the paper the concept of multidimensional global knowledge map, elaborated
through unsupervised extraction of dependencies from large documents corpus, is
proposed. In addition, the problem of direct Human - Knowledge Representation
System interface is addressed and a concept of adaptive decoder proposed for
the purpose of interaction with previously described unified mapping model. In
combination these two approaches are suggested as basis for a development of a
new generation of knowledge representation systems.
","Dmytro Filatov, Taras Filatov","Sat, 21 Feb 2015 18:29:57 GMT (167kb)",Artificial Intelligence (cs.AI),Unified vector space mapping for knowledge representation systems
"In computer interfaces in general, especially in information retrieval tasks,
it is important to be able to quickly find and retrieve information. State of
the art approach, used, for example, in search engines, is not effective as it
introduces losses of meanings due to context to keywords back and forth
translation. Authors argue it increases the time and reduces the accuracy of
information retrieval compared to what it could be in the system that employs
modern information retrieval and text mining methods while presenting results
in an adaptive human- computer interface where system effectively learns what
operator needs through iterative interaction. In current work, a combination of
adaptive navigational interface and real time collaborative feedback analysis
for documents relevance weighting is proposed as an viable alternative to
prevailing ""telegraphic"" approach in information retrieval systems. Adaptive
navigation is provided through a dynamic links panel controlled by an
evolutionary algorithm. Documents relevance is initially established with
standard information retrieval techniques and is further refined in real time
through interaction of users with the system. Introduced concepts of
multidimensional Knowledge Map and Weighted Point of Interest allow finding
relevant documents and users with common interests through a trivial
calculation. Browsing search approach, the ability of the algorithm to adapt
navigation to users interests, collaborative refinement and the self-organising
features of the system are the main factors making such architecture effective
in various fields where non-structured knowledge shall be represented to the
users.
","Dmytro Filatov, Taras Filatov","Thu, 19 Feb 2015 11:47:28 GMT (1619kb)",Information Retrieval (cs.IR),Evolutionary algorithm based adaptive navigation in information retrieval interfaces
"We address the problem of \emph{quantification}, a supervised learning task
whose goal is, given a class, to estimate the relative frequency (or
\emph{prevalence}) of the class in a dataset of unlabelled items.
Quantification has several applications in data and text mining, such as
estimating the prevalence of positive reviews in a set of reviews of a given
product, or estimating the prevalence of a given support issue in a dataset of
transcripts of phone calls to tech support. So far, quantification has been
addressed by learning a general-purpose classifier, counting the unlabelled
items which have been assigned the class, and tuning the obtained counts
according to some heuristics. In this paper we depart from the tradition of
using general-purpose classifiers, and use instead a supervised learning model
for \emph{structured prediction}, capable of generating classifiers directly
optimized for the (multivariate and non-linear) function used for evaluating
quantification accuracy. The experiments that we have run on 5500 binary
high-dimensional datasets (averaging more than 14,000 documents each) show that
this method is more accurate, more stable, and more efficient than existing,
state-of-the-art quantification methods.
","Andrea Esuli, Fabrizio Sebastiani","Thu, 19 Feb 2015 08:06:54 GMT (237kb,D) [v2] Wed, 15 Apr 2015 14:45:59 GMT (237kb,D)",Learning (cs.LG),Optimizing Text Quantifiers for Multivariate Loss Functions
"In modern societies, cultural change seems ceaseless. The flux of fashion is
especially obvious for popular music. While much has been written about the
origin and evolution of pop, most claims about its history are anecdotal rather
than scientific in nature. To rectify this we investigate the US Billboard Hot
100 between 1960 and 2010. Using Music Information Retrieval (MIR) and
text-mining tools we analyse the musical properties of ~17,000 recordings that
appeared in the charts and demonstrate quantitative trends in their harmonic
and timbral properties. We then use these properties to produce an audio-based
classification of musical styles and study the evolution of musical diversity
and disparity, testing, and rejecting, several classical theories of cultural
change. Finally, we investigate whether pop musical evolution has been gradual
or punctuated. We show that, although pop music has evolved continuously, it
did so with particular rapidity during three stylistic ""revolutions"" around
1964, 1983 and 1991. We conclude by discussing how our study points the way to
a quantitative science of cultural change.
","Matthias Mauch, Robert M. MacCallum, Mark Levy, Armand M. Leroi","Tue, 17 Feb 2015 18:32:39 GMT (6182kb,D)",Physics and Society (physics.soc-ph),The Evolution of Popular Music: USA 1960-2010
"Count-Min Sketch is a widely adopted algorithm for approximate event counting
in large scale processing. However, the original version of the
Count-Min-Sketch (CMS) suffers of some deficiences, especially if one is
interested by the low-frequency items, such as in text-mining related tasks.
Several variants of CMS have been proposed to compensate for the high relative
error for low-frequency events, but the proposed solutions tend to correct the
errors instead of preventing them. In this paper, we propose the Count-Min-Log
sketch, which uses logarithm-based, approximate counters instead of linear
counters to improve the average relative error of CMS at constant memory
footprint.
","Guillaume Pitel, Geoffroy Fouquier","Tue, 17 Feb 2015 13:17:17 GMT (30kb,D)",Information Retrieval (cs.IR),Count-Min-Log sketch: Approximately counting with approximate counters
"Most tools for accessing digitized historical newspapers emphasize relatively
simple search; but, as increasing numbers of digitized historical newspapers
and other historical resources become available we can consider much richer
modes of interaction with these collections. For instance, users might use
exploratory search for looking at larger issues and events such as elections
and campaigns or to get a sense of ""the texture of the city"" or ""what the city
was thinking"". To take full advantage of rich interface tools, the content of
the newspapers needs to be described systematically and accurately. Moreover,
collections of multiple newspapers need to be richly cross-indexed across
titles and even with historical resources beyond the newspapers.
",Robert B. Allen,"Fri, 13 Feb 2015 11:12:55 GMT (109kb)",Digital Libraries (cs.DL),"Improving Access to Digitized Historical Newspapers with Text Mining, Coordinated Models, and Formative User Interface Design"
"Applying traditional collaborative filtering to digital publishing is
challenging because user data is very sparse due to the high volume of
documents relative to the number of users. Content based approaches, on the
other hand, is attractive because textual content is often very informative. In
this paper we describe large-scale content based collaborative filtering for
digital publishing. To solve the digital publishing recommender problem we
compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets
(DBN) that both find low-dimensional latent representations for documents.
Efficient retrieval can be carried out in the latent representation. We work
both on public benchmarks and digital media content provided by Issuu, an
online publishing platform. This article also comes with a newly developed deep
belief nets toolbox for topic modeling tailored towards performance evaluation
of the DBN model and comparisons to the LDA model.
","Lars Maaloe, Morten Arngren, Ole Winther","Sun, 18 Jan 2015 17:12:59 GMT (6521kb,D)",Computation and Language (cs.CL),Deep Belief Nets for Topic Modeling
"The continuous increase of data generated provides enormous possibilities of
both public and private companies. The management of this mass of data or big
data will play a crucial role in the society of the future, as it finds
applications in different fields. There are so much potential and extremely
useful insights hidden in the huge volume of data. The advanced analysis
techniques available including predictive analytics, text mining, semantic
analysis are needed to enable organizations to create a competitive advantage
through data analyzed with different levels of sophistication, speed and
accuracy previously unavailable. Therefore, is it still possible to have that
level of sophistication with the ubiquitous numeric ocean that accompanies use
every day via connected devices that invade our lives? However, development of
big data requires a good understanding of the issues associated with it. And
this is the purpose of this paper, which focuses on giving a close-up view of
big data analysis, opportunities and challenges.
","Fatima El Jamiy, Abderrahmane Daif, Mohamed Azouazi, Abdelaziz Marzak","Wed, 14 Jan 2015 17:53:08 GMT (348kb)",Computers and Society (cs.CY),The potential and challenges of Big data - Recommendation systems next level application
"With the increase of information, document classification as one of the
methods of text mining, plays vital role in many management and organizing
information. Document classification is the process of assigning a document to
one or more predefined category labels. Document classification includes
different parts such as text processing, term selection, term weighting and
final classification. The accuracy of document classification is very
important. Thus improvement in each part of classification should lead to
better results and higher precision. Term weighting has a great impact on the
accuracy of the classification. Most of the existing weighting methods exploit
the statistical information of terms in documents and do not consider semantic
relations between words. In this paper, an automated document classification
system is presented that uses a novel term weighting method based on semantic
relations between terms. To evaluate the proposed method, three standard
Persian corpuses are used. Experiment results show 2 to 4 percent improvement
in classification accuracy compared with the best previous designed system for
Persian documents.
","Saeed Parseh, Ahmad Baraani","Sun, 28 Dec 2014 10:56:53 GMT (404kb)",Information Retrieval (cs.IR),Improving Persian Document Classification Using Semantic Relations between Words
"Principal Component Analysis (PCA) has wide applications in machine learning,
text mining and computer vision. Classical PCA based on a Gaussian noise model
is fragile to noise of large magnitude. Laplace noise assumption based PCA
methods cannot deal with dense noise effectively. In this paper, we propose
Cauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective
PCA method which is robust to various types of noise. We utilize Cauchy
distribution to model noise and derive Cauchy PCA under the maximum likelihood
estimation (MLE) framework with low rank constraint. Our method can robustly
estimate the low rank matrix regardless of whether noise is large or small,
dense or sparse. We analyze the robustness of Cauchy PCA from a robust
statistics view and present an efficient singular value projection optimization
method. Experimental results on both simulated data and real applications
demonstrate the robustness of Cauchy PCA to various noise patterns.
","Pengtao Xie, Eric Xing","Fri, 19 Dec 2014 20:06:02 GMT (1100kb,D)",Learning (cs.LG),Cauchy Principal Component Analysis
"Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a
subject of intense scientific interest. Biomedical literature mining can aid
DDI research by extracting evidence for large numbers of potential interactions
from published literature and clinical databases. Though DDI is investigated in
domains ranging in scale from intracellular biochemistry to human populations,
literature mining has not been used to extract specific types of experimental
evidence, which are reported differently for distinct experimental goals. We
focus on pharmacokinetic evidence for DDI, essential for identifying causal
mechanisms of putative interactions and as input for further pharmacological
and pharmaco-epidemiology investigations. We used manually curated corpora of
PubMed abstracts and annotated sentences to evaluate the efficacy of literature
mining on two tasks: first, identifying PubMed abstracts containing
pharmacokinetic evidence of DDIs; second, extracting sentences containing such
evidence from abstracts. We implemented a text mining pipeline and evaluated it
using several linear classifiers and a variety of feature transforms. The most
important textual features in the abstract and sentence classification tasks
were analyzed. We also investigated the performance benefits of using features
derived from PubMed metadata fields, various publicly available named entity
recognizers, and pharmacokinetic dictionaries. Several classifiers performed
very well in distinguishing relevant and irrelevant abstracts (reaching
F1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,
iAUC~=0.83). We found that word bigram features were important for achieving
optimal classifier performance and that features derived from Medical Subject
Headings (MeSH) terms significantly improved abstract classification. ...
","Artemy Kolchinsky, An?lia Louren?o, Heng-Yi Wu, Lang Li, Luis M. Rocha","Tue, 2 Dec 2014 00:01:39 GMT (967kb,D) [v2] Mon, 18 May 2015 16:45:42 GMT (621kb,D)",Machine Learning (stat.ML),Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from the Literature
"This paper describes a novel approach to learning term-weighting schemes
(TWSs) in the context of text classification. In text mining a TWS determines
the way in which documents will be represented in a vector space model, before
applying a classifier. Whereas acceptable performance has been obtained with
standard TWSs (e.g., Boolean and term-frequency schemes), the definition of
TWSs has been traditionally an art. Further, it is still a difficult task to
determine what is the best TWS for a particular problem and it is not clear
yet, whether better schemes, than those currently available, can be generated
by combining known TWS. We propose in this article a genetic program that aims
at learning effective TWSs that can improve the performance of current schemes
in text classification. The genetic program learns how to combine a set of
basic units to give rise to discriminative TWSs. We report an extensive
experimental study comprising data sets from thematic and non-thematic text
classification as well as from image classification. Our study shows the
validity of the proposed method; in fact, we show that TWSs learned with the
genetic program outperform traditional schemes and other TWSs proposed in
recent works. Further, we show that TWSs learned from a specific domain can be
effectively used for other tasks.
","Hugo Jair Escalante, Mauricio A. Garc?a-Lim?n, Alicia Morales-Reyes, Mario Graff, Manuel Montes-y-G?mez, Eduardo F. Morales","Thu, 2 Oct 2014 18:38:11 GMT (161kb) [v2] Fri, 3 Oct 2014 19:47:03 GMT (161kb) [v3] Mon, 6 Oct 2014 20:48:29 GMT (161kb)",Neural and Evolutionary Computing (cs.NE),Term-Weighting Learning via Genetic Programming for Text Classification
"This paper presents a goodness-of-fit test for parametric regression models
with scalar response and directional predictor, that is, vectors in a sphere of
arbitrary dimension. The testing procedure is based on the weighted squared
distance between a smooth and a parametric regression estimator, where the
smooth regression estimator is obtained by a projected local approach.
Asymptotic behavior of the test statistic under the null hypothesis and local
alternatives is provided, jointly with a consistent bootstrap algorithm for
application in practice. A simulation study illustrates the performance of the
test in finite samples. The procedure is also applied to a real data example
from text mining.
","Eduardo Garc?a-Portugu?s, Ingrid Van Keilegom, Rosa M. Crujeiras, Wenceslao Gonz?lez-Manteiga","Mon, 1 Sep 2014 19:01:12 GMT (3457kb,D) [v2] Sun, 28 Sep 2014 03:19:22 GMT (1862kb,D) [v3] Fri, 3 Apr 2015 15:08:59 GMT (1863kb,D)",Methodology (stat.ME),Testing parametric models in linear-directional regression
"Cluster analysis is a field of data analysis that extracts underlying
patterns in data. One application of cluster analysis is in text-mining, the
analysis of large collections of text to find similarities between documents.
We used a collection of about 30,000 tweets extracted from Twitter just before
the World Cup started. A common problem with real world text data is the
presence of linguistic noise. In our case it would be extraneous tweets that
are unrelated to dominant themes. To combat this problem, we created an
algorithm that combined the DBSCAN algorithm and a consensus matrix. This way
we are left with the tweets that are related to those dominant themes. We then
used cluster analysis to find those topics that the tweets describe. We
clustered the tweets using k-means, a commonly used clustering algorithm, and
Non-Negative Matrix Factorization (NMF) and compared the results. The two
algorithms gave similar results, but NMF proved to be faster and provided more
easily interpreted results. We explored our results using two visualization
tools, Gephi and Wordle.
","Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek","Thu, 21 Aug 2014 17:58:33 GMT (2293kb,D)",Machine Learning (stat.ML),A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets
"In the biclustering problem, we seek to simultaneously group observations and
features. While biclustering has applications in a wide array of domains,
ranging from text mining to collaborative filtering, the problem of identifying
structure in high dimensional genomic data motivates this work. In this
context, biclustering enables us to identify subsets of genes that are
co-expressed only within a subset of experimental conditions. We present a
convex formulation of the biclustering problem that possesses a unique global
minimizer and an iterative algorithm, COBRA, that is guaranteed to identify it.
Our approach generates an entire solution path of possible biclusters as a
single tuning parameter is varied. We also show how to reduce the problem of
selecting this tuning parameter to solving a trivial modification of the convex
biclustering problem. The key contributions of our work are its simplicity,
interpretability, and algorithmic guarantees - features that arguably are
lacking in the current alternative algorithms. We demonstrate the advantages of
our approach, which includes stably and reproducibly identifying biclusterings,
on simulated and real microarray data.
","Eric C. Chi, Genevera I. Allen, Richard G. Baraniuk","Tue, 5 Aug 2014 03:27:17 GMT (258kb,D) [v2] Thu, 25 Sep 2014 21:39:47 GMT (981kb,D) [v3] Fri, 29 Jan 2016 00:09:49 GMT (983kb,D) [v4] Fri, 15 Apr 2016 19:57:55 GMT (769kb,D)",Methodology (stat.ME),Convex Biclustering
"Neural network techniques are widely applied to obtain high-quality
distributed representations of words, i.e., word embeddings, to address text
mining, information retrieval, and natural language processing tasks. Recently,
efficient methods have been proposed to learn word embeddings from context that
captures both semantic and syntactic relationships between words. However, it
is challenging to handle unseen words or rare words with insufficient context.
In this paper, inspired by the study on word recognition process in cognitive
psychology, we propose to take advantage of seemingly less obvious but
essentially important morphological knowledge to address these challenges. In
particular, we introduce a novel neural network architecture called KNET that
leverages both contextual information and morphological word similarity built
based on morphological knowledge to learn word embeddings. Meanwhile, the
learning architecture is also able to refine the pre-defined morphological
knowledge and obtain more accurate word similarity. Experiments on an
analogical reasoning task and a word similarity task both demonstrate that the
proposed KNET framework can greatly enhance the effectiveness of word
embeddings.
","Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Tie-Yan Liu","Mon, 7 Jul 2014 12:45:10 GMT (220kb,D) [v2] Mon, 1 Sep 2014 16:03:47 GMT (317kb,D) [v3] Fri, 5 Sep 2014 15:58:35 GMT (317kb,D)",Computation and Language (cs.CL),KNET: A General Framework for Learning Word Embedding using Morphological Knowledge
"We revisit the problem of predicting directional movements of stock prices
based on news articles: here our algorithm uses daily articles from The Wall
Street Journal to predict the closing stock prices on the same day. We propose
a unified latent space model to characterize the ""co-movements"" between stock
prices and news articles. Unlike many existing approaches, our new model is
able to simultaneously leverage the correlations: (a) among stock prices, (b)
among news articles, and (c) between stock prices and news articles. Thus, our
model is able to make daily predictions on more than 500 stocks (most of which
are not even mentioned in any news article) while having low complexity. We
carry out extensive backtesting on trading strategies based on our algorithm.
The result shows that our model has substantially better accuracy rate (55.7%)
compared to many widely used algorithms. The return (56%) and Sharpe ratio due
to a trading strategy based on our model are also much higher than baseline
indices.
","Felix Ming Fai Wong, Zhenming Liu, Mung Chiang","Fri, 27 Jun 2014 22:34:47 GMT (398kb,D)",Learning (cs.LG),Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization
"Online forums enable users to discuss together around various topics. One of
the serious problems of these environments is high volume of discussions and
thus information overload problem. Unfortunately without considering the users
interests, traditional Information Retrieval (IR) techniques are not able to
solve the problem. Therefore, employment of a Recommender System (RS) that
could suggest favorite's topics of users according to their tastes could
increases the dynamism of forum and prevent the users from duplicate posts. In
addition, consideration of semantics can be useful for increasing the
performance of IR based RS. Our goal is study of impact of ontology and data
mining techniques on improving of content-based RS. For this purpose, at first,
three type of ontologies will be constructed from the domain corpus with
utilization of text mining, Natural Language Processing (NLP) and Wordnet and
then they will be used as an input in two kind of RS: one, fully ontology-based
and one with enriching the user profile vector with ontology in vector space
model (VSM) (proposed method). Afterward the results will be compared with the
simple VSM based RS. Given results show that the proposed RS presents the
highest performance.
","Hadi Fanaee-T, Mehran Yazdi","Thu, 12 Jun 2014 16:02:16 GMT (1792kb)",Information Retrieval (cs.IR),A Semantic VSM-Based Recommender System
"In this paper we propose a general framework for learning distributed
representations of attributes: characteristics of text whose representations
can be jointly learned with word embeddings. Attributes can correspond to
document indicators (to learn sentence vectors), language indicators (to learn
distributed language representations), meta-data and side information (such as
the age, gender and industry of a blogger) or representations of authors. We
describe a third-order model where word context and attribute vectors interact
multiplicatively to predict the next word in a sequence. This leads to the
notion of conditional word similarity: how meanings of words change when
conditioned on different attributes. We perform several experimental tasks
including sentiment classification, cross-lingual document classification, and
blog authorship attribution. We also qualitatively evaluate conditional word
neighbours and attribute-conditioned text generation.
","Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov","Tue, 10 Jun 2014 20:29:10 GMT (350kb,D)",Learning (cs.LG),A Multiplicative Model for Learning Distributed Text-Based Attribute Representations
"Service oriented architecture integrated with text mining allows services to
extract information in a well defined manner. In this paper, it is proposed to
design a knowledge extracting system for the Ocean Information Data System.
Deployed ARGO floating sensors of INCOIS (Indian National Council for Ocean
Information Systems) organization reflects the characteristics of ocean. This
is forwarded to the OIDS (Ocean Information Data System). For the data received
from OIDS, pre-processing techniques are applied. Pre-processing involves the
header retrieval and data separation. Header information is used to identify
the region of sensor, whereas data is used in the analysis process of Ocean
Information System. Analyzed data is segmented based on the region, by the
header value. Mining technique and composition principle is applied on the
segments for further analysis. Index Terms-- Service oriented architecture;
Text Mining; ARGO floating sensor; INCOIS; OIDS; Pre-processing.
","P. Ramya, S. Sasirekha","Sat, 7 Jun 2014 03:04:02 GMT (667kb)",Information Retrieval (cs.IR),Text Mining System for Non-Expert Miners
"Chinese characters have a complex and hierarchical graphical structure
carrying both semantic and phonetic information. We use this structure to
enhance the text model and obtain better results in standard NLP operations.
First of all, to tackle the problem of graphical variation we define
allographic classes of characters. Next, the relation of inclusion of a
subcharacter in a characters, provides us with a directed graph of allographic
classes. We provide this graph with two weights: semanticity (semantic relation
between subcharacter and character) and phoneticity (phonetic relation) and
calculate ""most semantic subcharacter paths"" for each character. Finally,
adding the information contained in these paths to unigrams we claim to
increase the efficiency of text mining methods. We evaluate our method on a
text classification task on two corpora (Chinese and Japanese) of a total of 18
million characters and get an improvement of 3% on an already high baseline of
89.6% precision, obtained by a linear SVM classifier. Other possible
applications and perspectives of the system are discussed.
",Yannis Haralambous,"Wed, 21 May 2014 16:49:50 GMT (2054kb)",Computation and Language (cs.CL),New Perspectives in Sinographic Language Processing Through the Use of Character Structure
"Stemming is a pre-processing step in Text Mining applications as well as a
very common requirement of Natural Language processing functions. Stemming is
the process for reducing inflected words to their stem. The main purpose of
stemming is to reduce different grammatical forms / word forms of a word like
its noun, adjective, verb, adverb etc. to its root form. Stemming is widely
uses in Information Retrieval system and reduces the size of index files. We
can say that the goal of stemming is to reduce inflectional forms and sometimes
derivationally related forms of a word to a common base form. In this paper we
have discussed different stemming algorithm for non-Indian and Indian language,
methods of stemming, accuracy and errors.
","Dalwadi Bijal, Suthar Sanket","Thu, 10 Apr 2014 17:16:01 GMT (264kb)",Computation and Language (cs.CL),Overview of Stemming Algorithms for Indian and Non-Indian Languages
"This paper proposes an information retrieval method for the economy news. The
effect of economy news, are researched in the word level and stock market
values are considered as the ground proof. The correlation between stock market
prices and economy news is an already addressed problem for most of the
countries. The most well-known approach is applying the text mining approaches
to the news and some time series analysis techniques over stock market closing
values in order to apply classification or clustering algorithms over the
features extracted. This study goes further and tries to ask the question what
are the available time series analysis techniques for the stock market closing
values and which one is the most suitable? In this study, the news and their
dates are collected into a database and text mining is applied over the news,
the text mining part has been kept simple with only term frequency-inverse
document frequency method. For the time series analysis part, we have studied
10 different methods such as random walk, moving average, acceleration,
Bollinger band, price rate of change, periodic average, difference, momentum or
relative strength index and their variation. In this study we have also
explained these techniques in a comparative way and we have applied the methods
over Turkish Stock Market closing values for more than a 2 year period. On the
other hand, we have applied the term frequency-inverse document frequency
method on the economy news of one of the high-circulating newspapers in Turkey.
","Sadi Evren Seker, Cihan Mert, Khaled Al-Naami, Nuri Ozalp, Ugur Ayan","Sat, 8 Mar 2014 19:50:15 GMT (363kb)","Computational Engineering, Finance, and Science (cs.CE)",Time Series Analysis on Stock Market for Text Mining Correlation of Economy News
"Rotationally symmetric distributions on the p-dimensional unit hypersphere,
extremely popular in directional statistics, involve a location parameter theta
that indicates the direction of the symmetry axis. The most classical way of
addressing the spherical location problem H_0:theta=theta_0, with theta_0 a
fixed location, is the so-called Watson test, which is based on the sample mean
of the observations. This test enjoys many desirable properties, but its
implementation requires the sample size n to be large compared to the dimension
p. This is a severe limitation, since more and more problems nowadays involve
high-dimensional directional data (e.g., in genetics or text mining). In this
work, we therefore introduce a modified Watson statistic that can cope with
high-dimensionality. We derive its asymptotic null distribution as both n and p
go to infinity. This is achieved in a universal asymptotic framework that
allows p to go to infinity arbitrarily fast (or slowly) as a function of n. We
further show that our results also provide high-dimensional tests for a problem
that has recently attracted much attention, namely that of testing that the
covariance matrix of a multinormal distribution has a ""theta_0-spiked""
structure. Finally, a Monte Carlo simulation study corroborates our asymptotic
results.
","Christophe Ley, Davy Paindaveine, Thomas Verdebout","Wed, 12 Feb 2014 13:56:55 GMT (546kb)",Statistics Theory (math.ST),High-dimensional tests for spherical location and spiked covariance
"Researchers and scientists increasingly find themselves in the position of
having to quickly understand large amounts of technical material. Our goal is
to effectively serve this need by using bibliometric text mining and
summarization techniques to generate summaries of scientific literature. We
show how we can use citations to produce automatically generated, readily
consumable, technical extractive summaries. We first propose C-LexRank, a model
for summarizing single scientific articles based on citations, which employs
community detection and extracts salient information-rich sentences. Next, we
further extend our experiments to summarize a set of papers, which cover the
same scientific topic. We generate extractive summaries of a set of Question
Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their
citation sentences and show that citations have unique information amenable to
creating a summary.
","Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, Taesun Moon","Tue, 4 Feb 2014 01:33:10 GMT (637kb)",Information Retrieval (cs.IR),Generating Extractive Summaries of Scientific Paradigms
"This study aims to publish a novel similarity metric to increase the speed of
comparison operations. Also the new metric is suitable for distance-based
operations among strings. Most of the simple calculation methods, such as
string length are fast to calculate but does not represent the string
correctly. On the other hand the methods like keeping the histogram over all
characters in the string are slower but good to represent the string
characteristics in some areas, like natural language. We propose a new metric,
easy to calculate and satisfactory for string comparison. Method is built on a
hash function, which gets a string at any size and outputs the most frequent K
characters with their frequencies. The outputs are open for comparison and our
studies showed that the success rate is quite satisfactory for the text mining
operations.
","Sadi Evren Seker, Oguz Altun, U?ur Ayan, Cihan Mert","Sat, 25 Jan 2014 23:40:46 GMT (930kb)",Data Structures and Algorithms (cs.DS),A Novel String Distance Function based on Most Frequent K Characters
"For marketing to function in a globalized world it must respect a diverse set
of local cultures. With marketing efforts extending to social media platforms,
the crossing of cultural boundaries can happen in an instant. In this paper we
examine how culture influences the popularity of marketing messages in social
media platforms. Text mining, automated translation and sentiment analysis
contribute largely to our research. From our analysis of 400 posts on the
localized Google+ pages of German car brands in Germany and the US, we conclude
that posting time and emotions are important predictors for reshare counts.
","Ronald Hochreiter, Christoph Waldhauser","Wed, 22 Jan 2014 16:41:27 GMT (57kb,D) [v2] Wed, 16 Apr 2014 16:05:10 GMT (603kb)",Social and Information Networks (cs.SI),Data Mining Cultural Aspects of Social Media Marketing
"Nonnegative matrix factorization (NMF) has become a widely used tool for the
analysis of high-dimensional data as it automatically extracts sparse and
meaningful features from a set of nonnegative data vectors. We first illustrate
this property of NMF on three applications, in image processing, text mining
and hyperspectral imaging --this is the why. Then we address the problem of
solving NMF, which is NP-hard in general. We review some standard NMF
algorithms, and also present a recent subclass of NMF problems, referred to as
near-separable NMF, that can be solved efficiently (that is, in polynomial
time), even in the presence of noise --this is the how. Finally, we briefly
describe some problems in mathematics and computer science closely related to
NMF via the nonnegative rank.
",Nicolas Gillis,"Tue, 21 Jan 2014 09:03:12 GMT (2058kb) [v2] Fri, 7 Mar 2014 10:32:43 GMT (2064kb)",Machine Learning (stat.ML),The Why and How of Nonnegative Matrix Factorization
"Multilingual text processing is useful because the information content found
in different languages is complementary, both regarding facts and opinions.
While Information Extraction and other text mining software can, in principle,
be developed for many languages, most text analysis tools have only been
applied to small sets of languages because the development effort per language
is large. Self-training tools obviously alleviate the problem, but even the
effort of providing training data and of manually tuning the results is usually
considerable. In this paper, we gather insights by various multilingual system
developers on how to minimise the effort of developing natural language
processing applications for many languages. We also explain the main guidelines
underlying our own effort to develop complex text mining software for tens of
languages. While these guidelines - most of all: extreme simplicity - can be
very restrictive and limiting, we believe to have shown the feasibility of the
approach through the development of the Europe Media Monitor (EMM) family of
applications (this http URL). EMM is a set of complex
media monitoring tools that process and analyse up to 100,000 online news
articles per day in between twenty and fifty languages. We will also touch upon
the kind of language resources that would make it easier for all to develop
highly multilingual text mining applications. We will argue that - to achieve
this - the most needed resources would be freely available, simple, parallel
and uniform multilingual dictionaries, corpora and software tools.
",Ralf Steinberger,"Mon, 13 Jan 2014 18:05:28 GMT (753kb)",Computation and Language (cs.CL),A survey of methods to ease the development of highly multilingual text mining applications
"The correlation and interactions among different biological entities comprise
the biological system. Although already revealed interactions contribute to the
understanding of different existing systems, researchers face many questions
everyday regarding inter-relationships among entities. Their queries have
potential role in exploring new relations which may open up a new area of
investigation. In this paper, we introduce a text mining based method for
answering the biological queries in terms of statistical computation such that
researchers can come up with new knowledge discovery. It facilitates user to
submit their query in natural linguistic form which can be treated as
hypothesis. Our proposed approach analyzes the hypothesis and measures the
p-value of the hypothesis with respect to the existing literature. Based on the
measured value, the system either accepts or rejects the hypothesis from
statistical point of view. Moreover, even it does not find any direct
relationship among the entities of the hypothesis, it presents a network to
give an integral overview of all the entities through which the entities might
be related. This is also congenial for the researchers to widen their view and
thus think of new hypothesis for further investigation. It assists researcher
to get a quantitative evaluation of their assumptions such that they can reach
a logical conclusion and thus aids in relevant re-searches of biological
knowledge discovery. The system also provides the researchers a graphical
interactive interface to submit their hypothesis for assessment in a more
convenient way.
","Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, Kazi Zakia Sultana","Thu, 9 Jan 2014 18:05:15 GMT (264kb)",Information Retrieval (cs.IR),Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery
"Machine learning for text classification is the underpinning of document
cataloging, news filtering, document steering and exemplification. In text
mining realm, effective feature selection is significant to make the learning
task more accurate and competent. One of the traditional lazy text classifier
k-Nearest Neighborhood (kNN) has a major pitfall in calculating the similarity
between all the objects in training and testing sets, there by leads to
exaggeration of both computational complexity of the algorithm and massive
consumption of main memory. To diminish these shortcomings in viewpoint of a
data-mining practitioner an amalgamative technique is proposed in this paper
using a novel restructured version of kNN called AugmentedkNN(AkNN) and
k-Medoids(kMdd) clustering.The proposed work comprises preprocesses on the
initial training set by imposing attribute feature selection for reduction of
high dimensionality, also it detects and excludes the high-fliers samples in
the initial training set and restructures a constrictedtraining set. The kMdd
clustering algorithm generates the cluster centers (as interior objects) for
each category and restructures the constricted training set with centroids.
This technique is amalgamated with AkNNclassifier that was prearranged with
text mining similarity measures. Eventually, significantweights and ranks were
assigned to each object in the new training set based upon their accessory
towards the object in testing set. Experiments conducted on Reuters-21578 a UCI
benchmark text mining data set, and comparisons with traditional kNNclassifier
designates the referredmethod yieldspreeminentrecitalin both clustering and
classification.
","RamachandraRao Kurada, Dr. K Karteeka Pavan","Mon, 9 Dec 2013 10:36:22 GMT (605kb)",Information Retrieval (cs.IR),Novel text categorization by amalgamation of augmented k-nearest neighborhood classification and k-medoids clustering
"As the growing interest of web recommendation systems those are applied to
deliver customized data for their users, we started working on this system.
Generally the recommendation systems are divided into two major categories such
as collaborative recommendation system and content based recommendation system.
In case of collaborative recommen-dation systems, these try to seek out users
who share same tastes that of given user as well as recommends the websites
according to the liking given user. Whereas the content based recommendation
systems tries to recommend web sites similar to those web sites the user has
liked. In the recent research we found that the efficient technique based on
asso-ciation rule mining algorithm is proposed in order to solve the problem of
web page recommendation. Major problem of the same is that the web pages are
given equal importance. Here the importance of pages changes according to the
fre-quency of visiting the web page as well as amount of time user spends on
that page. Also recommendation of newly added web pages or the pages those are
not yet visited by users are not included in the recommendation set. To
over-come this problem, we have used the web usage log in the adaptive
association rule based web mining where the asso-ciation rules were applied to
personalization. This algorithm was purely based on the Apriori data mining
algorithm in order to generate the association rules. However this method also
suffers from some unavoidable drawbacks. In this paper we are presenting and
investigating the new approach based on weighted Association Rule Mining
Algorithm and text mining. This is improved algorithm which adds semantic
knowledge to the results, has more efficiency and hence gives better quality
and performances as compared to existing approaches.
","Ujwala Wanaskar, Sheetal Vij, Debajyoti Mukhopadhyay","Thu, 28 Nov 2013 04:22:55 GMT (451kb)",Information Retrieval (cs.IR),A Hybrid Web Recommendation System based on the Improved Association Rule Mining Algorithm
"Principal component analysis (PCA) and related techniques have been
successfully employed in natural language processing. Text mining applications
in the age of the online social media (OSM) face new challenges due to
properties specific to these use cases (e.g. spelling issues specific to texts
posted by users, the presence of spammers and bots, service announcements,
etc.). In this paper, we employ a Robust PCA technique to separate typical
outliers and highly localized topics from the low-dimensional structure present
in language use in online social networks. Our focus is on identifying
geospatial features among the messages posted by the users of the Twitter
microblogging service. Using a dataset which consists of over 200 million
geolocated tweets collected over the course of a year, we investigate whether
the information present in word usage frequencies can be used to identify
regional features of language use and topics of interest. Using the PCA pursuit
method, we are able to identify important low-dimensional features, which
constitute smoothly varying functions of the geographic location.
","D?niel Kondor, Istv?n Csabai, L?szl? Dobos, J?nos Sz?le, Norbert Barankai, Tam?s Hanyecz, Tam?s Seb?k, Zs?fia Kallus, G?bor Vattay","Tue, 5 Nov 2013 19:31:33 GMT (1581kb,D)",Computation and Language (cs.CL),Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages
"Many of quality approaches are described in hundreds of textual pages. Manual
processing of information consumes plenty of resources. In this report we
present a text mining approach applied on CMMI, one well known and widely known
quality approach. The text mining analysis can provide a quick overview on the
scope of a quality approaches. The result of the analysis could accelerate the
understanding and the selection of quality approaches.
","Z?dor D?niel Kelemen, Rob Kusters, Jos Trienekens, Katalin Balla","Tue, 22 Oct 2013 17:14:27 GMT (600kb) [v2] Mon, 11 Nov 2013 18:38:57 GMT (0kb,I)",Software Engineering (cs.SE),Towards Applying Text Mining Techniques on Software Quality Standards and Models
"The riots in Stockholm in May 2013 were an event that reverberated in the
world media for its dimension of violence that had spread through the Swedish
capital. In this study we have investigated the role of social media in
creating media phenomena via text mining and natural language processing. We
have focused on two channels of communication for our analysis: Twitter and
Poloniainfo.se (Forum of Polish community in Sweden). Our preliminary results
show some hot topics driving discussion related mostly to Swedish Police and
Swedish Politics by counting word usage. Typical features for media
intervention are presented. We have built networks of most popular phrases,
clustered by categories (geography, media institution, etc.). Sentiment
analysis shows negative connotation with Police. The aim of this preliminary
exploratory quantitative study was to generate questions and hypotheses, which
we could carefully follow by deeper more qualitative methods.
","Andrzej Jarynowski, Amir Rostami","Fri, 4 Oct 2013 13:04:45 GMT (702kb)",Social and Information Networks (cs.SI),Reading Stockholm Riots 2013 in social media by text-mining
"Data reflecting social and business relations has often form of network of
connections between entities (called social network). In such network important
and influential users can be identified as well as groups of strongly connected
users. Finding such groups and observing their evolution becomes an
increasingly important research problem. One of the significant problems is to
develop method incorporating not only information about connections between
entities but also information obtained from text written by the users. Method
presented in this paper combine social network analysis and text mining in
order to understand groups evolution.
","Bogdan Gliwa, Anna Zygmunt, Stanisaw Podg?rski","Thu, 22 Aug 2013 21:41:18 GMT (550kb,D)",Social and Information Networks (cs.SI),Incorporating Text Analysis into Evolution of Social Groups in Blogosphere
"The Clinical E-Science Framework (CLEF) project was used to extract important
information from medical texts by building a system for the purpose of clinical
research, evidence-based healthcare and genotype-meets-phenotype informatics.
The system is divided into two parts, one part concerns with the identification
of relationships between clinically important entities in the text. The full
parses and domain-specific grammars had been used to apply many approaches to
extract the relationship. In the second part of the system, statistical machine
learning (ML) approaches are applied to extract relationship. A corpus of
oncology narratives that hand annotated with clinical relationships can be used
to train and test a system that has been designed and implemented by supervised
machine learning (ML) approaches. Many features can be extracted from these
texts that are used to build a model by the classifier. Multiple supervised
machine learning algorithms can be applied for relationship extraction. Effects
of adding the features, changing the size of the corpus, and changing the type
of the algorithm on relationship extraction are examined. Keywords: Text
mining; information extraction; NLP; entities; and relations.
","Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, Mohamed Monier Hassan","Fri, 21 Jun 2013 15:30:09 GMT (613kb)",Information Retrieval (cs.IR),Clinical Relationships Extraction Techniques from Patient Narratives
"Fuzzy logic deals with degrees of truth. In this paper, we have shown how to
apply fuzzy logic in text mining in order to perform document clustering. We
took an example of document clustering where the documents had to be clustered
into two categories. The method involved cleaning up the text and stemming of
words. Then, we chose m number of features which differ significantly in their
word frequencies (WF), normalized by document length, between documents
belonging to these two clusters. The documents to be clustered were represented
as a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was
used to cluster these documents into two clusters. After the FCM execution
finished, the documents in the two clusters were analysed for the values of
their respective m features. It was known that documents belonging to a
document type, say X, tend to have higher WF values for some particular
features. If the documents belonging to a cluster had higher WF values for
those same features, then that cluster was said to represent X. By fuzzy logic,
we not only get the cluster name, but also the degree to which a document
belongs to a cluster.
","Sumit Goswami, Mayank Singh Shishodia","Thu, 6 Jun 2013 07:35:23 GMT (359kb)",Learning (cs.LG),A Fuzzy Based Approach to Text Mining and Document Clustering
"The fourth international workshop on Computational Models for Cell Processes
(CompMod 2013) took place on June 11, 2013 at the {\AA}bo Akademi University,
Turku, Finland, in conjunction with iFM 2013. The first edition of the workshop
(2008) took place in Turku, Finland, in conjunction with Formal Methods 2008,
the second edition (2009) took place in Eindhoven, the Netherlands, as well in
conjunction with Formal Methods 2009, and the third one took place in Aachen,
Germany, in conjunction with CONCUR 2013. This volume contains the final
versions of all contributions accepted for presentation at the workshop.
The goal of the CompMod workshop series is to bring together researchers in
Computer Science and Mathematics (both discrete and continuous), interested in
the opportunities and the challenges of Systems Biology. The Program Committee
of CompMod 2013 selected 3 papers for presentation at the workshop. In
addition, we had two invited talks and five informal presentations.
The scientific program of the workshop spans an interesting mix of approaches
to systems and even synthetic biology, encompassing several different modeling
approaches, ranging from quantitative to qualitative techniques, from
continuous to discrete mathematics, and from deterministic to stochastic
methods. We thank our invited speakers Daniela Besozzi (Universita degli Studi
di Milano, Milano, Italy) and Juho Rousu (Aalto University, Finland) for
accepting our invitation and for presenting some of their recent results at
CompMod 2013.
The technical contributions address the mathematical modeling of the PDGF
signalling pathway, the canonical labelling of site graphs, rule-based modeling
of polymerization reactions, rule-based modeling as a platform for the analysis
of synthetic self-assembled nano-systems, robustness analysis of stochastic
systems, an algebraic approach to gene assembly in ciliates, and large-scale
text mining of biomedical literature.
",Ion Petre (?bo Akademi University),"Sun, 9 Jun 2013 13:42:56 GMT (267kb)","Computational Engineering, Finance, and Science (cs.CE)",Proceedings Fourth International Workshop on Computational Models for Cell Processes
"Somoclu is a massively parallel tool for training self-organizing maps on
large data sets written in C++. It builds on OpenMP for multicore execution,
and on MPI for distributing the workload across the nodes in a cluster. It is
also able to boost training by using CUDA if graphics processing units are
available. A sparse kernel is included, which is useful for high-dimensional
but sparse data, such as the vector spaces common in text mining workflows.
Python, R and MATLAB interfaces facilitate interactive use. Apart from fast
execution, memory use is highly optimized, enabling training large emergent
maps even on a single computer.
","Peter Wittek, Shi Chao Gao, Ik Soo Lim, Li Zhao","Tue, 7 May 2013 06:43:26 GMT (15kb,D) [v2] Wed, 28 Jan 2015 12:40:08 GMT (721kb,D) [v3] Mon, 11 Jan 2016 10:48:52 GMT (1712kb,D)","Distributed, Parallel, and Cluster Computing (cs.DC)",Somoclu: An Efficient Parallel Library for Self-Organizing Maps
"The Information and Communication Technologies revolution brought a digital
world with huge amounts of data available. Enterprises use mining technologies
to search vast amounts of data for vital insight and knowledge. Mining tools
such as data mining, text mining, and web mining are used to find hidden
knowledge in large databases or the Internet.
",Abdul-Aziz Rashid Al-Azmi,"Fri, 12 Apr 2013 08:04:31 GMT (321kb)",Information Retrieval (cs.IR),"Data, text and web mining for business intelligence: a survey"
"A major computational burden, while performing document clustering, is the
calculation of similarity measure between a pair of documents. Similarity
measure is a function that assigns a real number between 0 and 1 to a pair of
documents, depending upon the degree of similarity between them. A value of
zero means that the documents are completely dissimilar whereas a value of one
indicates that the documents are practically identical. Traditionally,
vector-based models have been used for computing the document similarity. The
vector-based models represent several features present in documents. These
approaches to similarity measures, in general, cannot account for the semantics
of the document. Documents written in human languages contain contexts and the
words used to describe these contexts are generally semantically related.
Motivated by this fact, many researchers have proposed seman-tic-based
similarity measures by utilizing text annotation through external thesauruses
like WordNet (a lexical database). In this paper, we define a semantic
similarity measure based on documents represented in topic maps. Topic maps are
rapidly becoming an industrial standard for knowledge representation with a
focus for later search and extraction. The documents are transformed into a
topic map based coded knowledge and the similarity between a pair of documents
is represented as a correlation between the common patterns (sub-trees). The
experimental studies on the text mining datasets reveal that this new
similarity measure is more effective as compared to commonly used similarity
measures in text clustering.
","Muhammad Rafi, Mohammad Shahid Shaikh","Sun, 17 Mar 2013 18:28:02 GMT (250kb)",Information Retrieval (cs.IR),An improved semantic similarity measure for document clustering based on topic maps
"Keyphrases are the phrases, consisting of one or more words, representing the
important concepts in the articles. Keyphrases are useful for a variety of
tasks such as text summarization, automatic indexing,
clustering/classification, text mining etc. This paper presents a hybrid
approach to keyphrase extraction from medical documents. The keyphrase
extraction approach presented in this paper is an amalgamation of two methods:
the first one assigns weights to candidate keyphrases based on an effective
combination of features such as position, term frequency, inverse document
frequency and the second one assign weights to candidate keyphrases using some
knowledge about their similarities to the structure and characteristics of
keyphrases available in the memory (stored list of keyphrases). An efficient
candidate keyphrase identification method as the first component of the
proposed keyphrase extraction system has also been introduced in this paper.
The experimental results show that the proposed hybrid approach performs better
than some state-of-the art keyphrase extraction approaches.
",Kamal Sarkar,"Wed, 6 Mar 2013 20:09:05 GMT (303kb) [v2] Sat, 25 Jan 2014 16:34:35 GMT (243kb)",Information Retrieval (cs.IR),A Hybrid Approach to Extract Keyphrases from Medical Documents
"Multi-way data arises in many applications such as electroencephalography
(EEG) classification, face recognition, text mining and hyperspectral data
analysis. Tensor decomposition has been commonly used to find the hidden
factors and elicit the intrinsic structures of the multi-way data. This paper
considers sparse nonnegative Tucker decomposition (NTD), which is to decompose
a given tensor into the product of a core tensor and several factor matrices
with sparsity and nonnegativity constraints. An alternating proximal gradient
method (APG) is applied to solve the problem. The algorithm is then modified to
sparse NTD with missing values. Per-iteration cost of the algorithm is
estimated scalable about the data size, and global convergence is established
under fairly loose conditions. Numerical experiments on both synthetic and real
world data demonstrate its superiority over a few state-of-the-art methods for
(sparse) NTD from partial and/or full observations. The MATLAB code along with
demos are accessible from the author's homepage.
",Yangyang Xu,"Mon, 11 Feb 2013 18:22:33 GMT (191kb,D) [v2] Tue, 6 May 2014 16:30:25 GMT (176kb,D)",Optimization and Control (math.OC),Alternating proximal gradient method for sparse nonnegative Tucker decomposition
"In text mining, information retrieval, and machine learning, text documents
are commonly represented through variants of sparse Bag of Words (sBoW) vectors
(e.g. TF-IDF). Although simple and intuitive, sBoW style representations suffer
from their inherent over-sparsity and fail to capture word-level synonymy and
polysemy. Especially when labeled data is limited (e.g. in document
classification), or the text documents are short (e.g. emails or abstracts),
many features are rarely observed within the training corpus. This leads to
overfitting and reduced generalization accuracy. In this paper we propose Dense
Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW
document features. dCoT explicitly models absent words by removing and
reconstructing random sub-sets of words in the unlabeled corpus. With this
approach, dCoT learns to reconstruct frequent words from co-occurring
infrequent words and maps the high dimensional sparse sBoW vectors into a
low-dimensional dense representation. We show that the feature removal can be
marginalized out and that the reconstruction can be solved for in closed-form.
We demonstrate empirically, on several benchmark datasets, that dCoT features
significantly improve the classification accuracy across several document
classification tasks.
","Zhixiang (Eddie)Xu, Minmin Chen, Kilian Q. Weinberger, Fei Sha","Mon, 28 Jan 2013 21:04:45 GMT (633kb)",Information Retrieval (cs.IR),An alternative text representation to TF-IDF and Bag-of-Words
"Non-negative matrix factorization (NMF) has become a popular machine learning
approach to many problems in text mining, speech and image processing,
bio-informatics and seismic data analysis to name a few. In NMF, a matrix of
non-negative data is approximated by the low-rank product of two matrices with
non-negative entries. In this paper, the approximation quality is measured by
the Kullback-Leibler divergence between the data and its low-rank
reconstruction. The existence of the simple multiplicative update (MU)
algorithm for computing the matrix factors has contributed to the success of
NMF. Despite the availability of algorithms showing faster convergence, MU
remains popular due to its simplicity. In this paper, a diagonalized Newton
algorithm (DNA) is proposed showing faster convergence while the implementation
remains simple and suitable for high-rank problems. The DNA algorithm is
applied to various publicly available data sets, showing a substantial speed-up
on modern hardware.
",Hugo Van hamme,"Tue, 15 Jan 2013 15:59:46 GMT (249kb) [v2] Mon, 18 Mar 2013 09:15:29 GMT (251kb)",Numerical Analysis (cs.NA),The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization
"Matrix approximation is a common tool in machine learning for building
accurate prediction models for recommendation systems, text mining, and
computer vision. A prevalent assumption in constructing matrix approximations
is that the partially observed matrix is of low-rank. We propose a new matrix
approximation model where we assume instead that the matrix is only locally of
low-rank, leading to a representation of the observed matrix as a weighted sum
of low-rank matrices. We analyze the accuracy of the proposed local low-rank
modeling. Our experiments show improvements in prediction accuracy in
recommendation tasks.
","Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer","Tue, 15 Jan 2013 00:54:38 GMT (313kb,D)",Learning (cs.LG),Matrix Approximation under Local Low-Rank Assumption
"Representation of semantic information contained in the words is needed for
any Arabic Text Mining applications. More precisely, the purpose is to better
take into account the semantic dependencies between words expressed by the
co-occurrence frequencies of these words. There have been many proposals to
compute similarities between words based on their distributions in contexts. In
this paper, we compare and contrast the effect of two preprocessing techniques
applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)
approaches for measuring the similarity between Arabic words with the well
known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of
distance functions and similarity measures, such as the Euclidean Distance,
Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation
Coefficient. The obtained results show that, on the one hand, the variety of
the corpus produces more accurate results; on the other hand, the Stem-based
approach outperformed the Root-based one because this latter affects the words
meanings.
","Hanane Froud, Abdelmonaim Lachkar, Said Alaoui Ouatik","Fri, 14 Dec 2012 23:34:07 GMT (250kb)",Computation and Language (cs.CL),A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications
"What characterizes influential users in online health communities (OHCs)? We
hypothesize that (1) the emotional support received by OHC members can be
assessed from their sentiment ex-pressed in online interactions, and (2) such
assessments can help to identify influential OHC members. Through text mining
and sentiment analysis of users' online interactions, we propose a novel metric
that directly measures a user's ability to affect the sentiment of others.
Using dataset from an OHC, we demonstrate that this metric is highly effective
in identifying influential users. In addition, combining the metric with other
traditional measures further improves the identification of influential users.
This study can facilitate online community management and advance our
understanding of social influence in OHCs.
","Kang Zhao, Greta Greer, Baojun Qiu, Prasenjit Mitra, Kenneth Portier, John Yen","Mon, 26 Nov 2012 20:37:00 GMT (562kb) [v2] Wed, 5 Dec 2012 23:12:05 GMT (563kb)",Social and Information Networks (cs.SI),Finding influential users of an online health community: a new metric based on sentiment influence
"The criminal nature of narcotics complicates the direct assessment of a drug
community, while having a good understanding of the type of people drawn or
currently using drugs is vital for finding effective intervening strategies.
Especially for the Russian Federation this is of immediate concern given the
dramatic increase it has seen in drug abuse since the fall of the Soviet Union
in the early nineties. Using unique data from the Russian social network
'LiveJournal' with over 39 million registered users worldwide, we were able for
the first time to identify the on-line drug community by context sensitive text
mining of the users' blogs using a dictionary of known drug-related official
and 'slang' terminology. By comparing the interests of the users that most
actively spread information on narcotics over the network with the interests of
the individuals outside the on-line drug community, we found that the 'average'
drug user in the Russian Federation is generally mostly interested in topics
such as Russian rock, non-traditional medicine, UFOs, Buddhism, yoga and the
occult. We identify three distinct scale-free sub-networks of users which can
be uniquely classified as being either 'infectious', 'susceptible' or 'immune'.
","L. J. Dijkstra, A. V. Yakushev, P. A. C. Duijn, A. V. Boukhanovsky, P. M. A. Sloot","Tue, 20 Nov 2012 15:46:35 GMT (1765kb) [v2] Mon, 10 Dec 2012 09:39:30 GMT (1766kb) [v3] Mon, 13 May 2013 16:02:14 GMT (1766kb)",Social and Information Networks (cs.SI),Inference of the Russian drug community from one of the largest social networks in the Russian Federation
"This paper introduces a new similarity measure based on edge counting in a
taxonomy like WorldNet or Ontology. Measurement of similarity between text
segments or concepts is very useful for many applications like information
retrieval, ontology matching, text mining, and question answering and so on.
Several measures have been developed for measuring similarity between two
concepts: out of these we see that the measure given by Wu and Palmer [1] is
simple, and gives good performance. Our measure is based on their measure but
strengthens it. Wu and Palmer [1] measure has a disadvantage that it does not
consider how far the concepts are semantically. In our measure we include the
shortest path between the concepts and the depth of whole taxonomy together
with the distances used in Wu and Palmer [1]. Also the measure has following
disadvantage i.e. in some situations, the similarity of two elements of an IS-A
ontology contained in the neighborhood exceeds the similarity value of two
elements contained in the same hierarchy. Our measure introduces a penalization
factor for this case based upon shortest length between the concepts and depth
of whole taxonomy.
","Manjula Shenoy.K, K.C.Shet, U.Dinesh Acharya","Tue, 20 Nov 2012 10:53:22 GMT (154kb)",Artificial Intelligence (cs.AI),A New Similarity Measure for Taxonomy Based on Edge Counting
"The term legal research generally refers to the process of identifying and
retrieving appropriate information necessary to support legal decision making
from past case records. At present, the process is mostly manual, but some
traditional technologies such as keyword searching are commonly used to speed
the process up. But a keyword search is not a comprehensive search to cater to
the requirements of legal research as the search result includes too many false
hits in terms of irrelevant case records. Hence the present generic tools
cannot be used to automate legal research.
This paper presents a framework which was developed by combining several Text
Mining techniques to automate the process overcoming the difficulties in the
existing methods. Further, the research also identifies the possible
enhancements that could be done to enhance the effectiveness of the framework.
",Mohamed Firdhous,"Thu, 8 Nov 2012 14:19:49 GMT (424kb)",Information Retrieval (cs.IR),Automating Legal Research through Data Mining
"In many real-world applications such as text mining, it is desirable to
select the most relevant features or variables to improve the generalization
ability, or to provide a better interpretation of the prediction models. {In
this paper, a novel adaptive feature scaling (AFS) scheme is proposed by
introducing a feature scaling {vector $\d \in [0, 1]^m$} to alleviate the bias
problem brought by the scaling bias of the diverse features.} By reformulating
the resultant AFS model to semi-infinite programming problem, a novel feature
generating method is presented to identify the most relevant features for
classification problems. In contrast to the traditional feature selection
methods, the new formulation has the advantage of solving extremely
high-dimensional and large-scale problems. With an exact solution to the
worst-case analysis in the identification of relevant features, the proposed
feature generating scheme converges globally. More importantly, the proposed
scheme facilitates the group selection with or without special structures.
Comprehensive experiments on a wide range of synthetic and real-world datasets
demonstrate that the proposed method {achieves} better or competitive
performance compared with the existing methods on (group) feature selection in
terms of generalization performance and training efficiency. The C++ and MATLAB
implementations of our algorithm can be available at
\emph{this http URL}.
","Mingkui Tan, Ivor W. Tsang, Li Wang","Mon, 24 Sep 2012 13:23:39 GMT (342kb)",Learning (cs.LG),Towards Large-scale and Ultrahigh Dimensional Feature Selection via Feature Generation
"A major computational burden, while performing document clustering, is the
calculation of similarity measure between a pair of documents. Similarity
measure is a function that assign a real number between 0 and 1 to a pair of
documents, depending upon the degree of similarity between them. A value of
zero means that the documents are completely dissimilar whereas a value of one
indicates that the documents are practically identical. Traditionally,
vector-based models have been used for computing the document similarity. The
vector-based models represent several features present in documents. These
approaches to similarity measures, in general, cannot account for the semantics
of the document. Documents written in human languages contain contexts and the
words used to describe these contexts are generally semantically related.
Motivated by this fact, many researchers have proposed semantic-based
similarity measures by utilizing text annotation through external thesauruses
like WordNet (a lexical database). In this paper, we define a semantic
similarity measure based on documents represented in topic maps. Topic maps are
rapidly becoming an industrial standard for knowledge representation with a
focus for later search and extraction. The documents are transformed into a
topic map based coded knowledge and the similarity between a pair of documents
is represented as a correlation between the common patterns. The experimental
studies on the text mining datasets reveal that this new similarity measure is
more effective as compared to commonly used similarity measures in text
clustering.
","Muhammad Rafi, Sundus Hassan, Mohammad Shahid Shaikh","Fri, 17 Aug 2012 15:49:38 GMT (438kb)",Information Retrieval (cs.IR),Content-based Text Categorization using Wikitology
"In this research, we propose a method to trace scientists' research trends
realtimely. By monitoring the downloads of scientific articles in the journal
of Scientometrics for 744 hours, namely one month, we investigate the download
statistics. Then we aggregate the keywords in these downloaded research papers,
and analyze the trends of article downloading and keyword downloading.
Furthermore, taking both the download of keywords and articles into
consideration, we design a method to detect the emerging research trends. We
find that in scientometrics field, social media, new indices to quantify
scientific productivity (g-index), webometrics, semantic, text mining, open
access are emerging fields that scientometrics researchers are focusing on.
","Xianwen Wang, Zhi Wang, Shenmeng Xu","Tue, 7 Aug 2012 07:08:21 GMT (778kb) [v2] Thu, 13 Dec 2012 05:36:34 GMT (916kb)",Digital Libraries (cs.DL),Tracing scientist's research trends realtimely
"The tremendous expanse of search engines, dictionary and thesaurus storage,
and other text mining applications, combined with the popularity of readily
available scanning devices and optical character recognition tools, has
necessitated efficient storage, retrieval and management of massive text
databases for various modern applications. For such applications, we propose a
novel data structure, INSTRUCT, for efficient storage and management of
sequence databases. Our structure uses bit vectors for reusing the storage
space for common triplets, and hence, has a very low memory requirement.
INSTRUCT efficiently handles prefix and suffix search queries in addition to
the exact string search operation by iteratively checking the presence of
triplets. We also propose an extension of the structure to handle substring
search efficiently, albeit with an increase in the space requirements. This
extension is important in the context of trie-based solutions which are unable
to handle such queries efficiently. We perform several experiments portraying
that INSTRUCT outperforms the existing structures by nearly a factor of two in
terms of space requirements, while the query times are better. The ability to
handle insertion and deletion of strings in addition to supporting all kinds of
queries including exact search, prefix/suffix search and substring search makes
INSTRUCT a complete data structure.
","Sourav Dutta, Arnab Bhattacharya","Mon, 2 Jul 2012 12:38:47 GMT (67kb,D) [v2] Tue, 3 Jul 2012 04:54:37 GMT (67kb,D)",Databases (cs.DB),INSTRUCT: Space-Efficient Structure for Indexing and Complete Query Management of String Databases
"As one of the simplest probabilistic topic modeling techniques, latent
Dirichlet allocation (LDA) has found many important applications in text
mining, computer vision and computational biology. Recent training algorithms
for LDA can be interpreted within a unified message passing framework. However,
message passing requires storing previous messages with a large amount of
memory space, increasing linearly with the number of documents or the number of
topics. Therefore, the high memory usage is often a major problem for topic
modeling of massive corpora containing a large number of topics. To reduce the
space complexity, we propose a novel algorithm without storing previous
messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP
relates the message passing algorithms with the non-negative matrix
factorization (NMF) algorithms, which absorb the message updating into the
message passing process, and thus avoid storing previous messages. Experimental
results on four large data sets confirm that TBP performs comparably well or
even better than current state-of-the-art training algorithms for LDA but with
a much less memory consumption. TBP can do topic modeling when massive corpora
cannot fit in the computer memory, for example, extracting thematic topics from
7 GB PUBMED corpora on a common desktop computer with 2GB memory.
","Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao","Wed, 6 Jun 2012 08:34:43 GMT (2235kb) [v2] Fri, 8 Jun 2012 14:07:26 GMT (2239kb)",Learning (cs.LG),Memory-Efficient Topic Modeling
"Many words in documents recur very frequently but are essentially meaningless
as they are used to join words together in a sentence. It is commonly
understood that stop words do not contribute to the context or content of
textual documents. Due to their high frequency of occurrence, their presence in
text mining presents an obstacle to the understanding of the content in the
documents. To eliminate the bias effects, most text mining software or
approaches make use of stop words list to identify and remove those words.
However, the development of such top words list is difficult and inconsistent
between textual sources. This problem is further aggravated by sources such as
Twitter which are highly repetitive or similar in nature. In this paper, we
will be examining the original work using term frequency, inverse document
frequency and term adjacency for developing a stop words list for the Twitter
data source. We propose a new technique using combinatorial values as an
alternative measure to effectively list out stop words.
",Murphy Choy,"Tue, 29 May 2012 15:37:46 GMT (234kb)",Information Retrieval (cs.IR),Effective Listings of Function Stop words for Twitter
"Web 2.0 applications like Twitter or Facebook create a continuous stream of
information. This demands new ways of analysis in order to offer insight into
this stream right at the moment of the creation of the information, because
lots of this data is only relevant within a short period of time. To address
this problem real time search engines have recently received increased
attention. They take into account the continuous flow of information
differently than traditional web search by incorporating temporal and social
features, that describe the context of the information during its creation.
Standard approaches where data first get stored and then is processed from a
peristent storage suffer from latency. We want to address the fluent and rapid
nature of text stream by providing an event based approach that analyses
directly the stream of information. In a first step we want to define the
difference between real time search and traditional search to clarify the
demands in modern text filtering. In a second step we want to show how event
based features can be used to support the tasks of real time search engines.
Using the example of Twitter we present in this paper a way how to combine an
event based approach with text mining and information filtering concepts in
order to classify incoming information based on stream features. We calculate
stream dependant features and feed them into a neural network in order to
classify the text streams. We show the separative capabilities of event based
features as the foundation for a real time search engine.
","Andreas Bauer, Christian Wolff","Mon, 16 Apr 2012 04:50:13 GMT (188kb) [v2] Fri, 28 Jun 2013 12:31:37 GMT (187kb)",Information Retrieval (cs.IR),Event based classification of Web 2.0 text streams
"Frequently asked questions (FAQs) are a popular way to document software
development knowledge. As creating such documents is expensive, this paper
presents an approach for automatically extracting FAQs from sources of software
development discussion, such as mailing lists and Internet forums, by combining
techniques of text mining and natural language processing. We apply the
approach to popular mailing lists and carry out a survey among software
developers to show that it is able to extract high-quality FAQs that may be
further improved by experts.
","Stefan Hen, Martin Monperrus (INRIA Lille - Nord Europe), Mira Mezini","Fri, 23 Mar 2012 07:13:06 GMT (184kb)",Software Engineering (cs.SE),Semi-Automatically Extracting FAQs to Improve Accessibility of Software Development Knowledge
"Literature-based knowledge discovery method was introduced by Dr. Swanson in
1986. He hypothesized a connection between Raynaud's phenomenon and dietary
fish oil, the field of literature-based discovery (LBD) was born from then on.
During the subsequent two decades, LBD's research attracts some scientists
including information science, computer science, and biomedical science, etc..
It has been a part of knowledge discovery and text mining. This paper
summarizes the development of recent years about LBD and presents two parts,
methodology research and applied research. Lastly, some problems are pointed as
future research directions.
","Xiaoyong Liu, Hui Fu","Fri, 16 Mar 2012 03:33:06 GMT (132kb)",Digital Libraries (cs.DL),Literature-based knowledge discovery: the state of the art
"Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a
toolbox for gaining new knowledge from unstructured text data. At the core of
CORDIET is the C-K theory which captures the essential elements of innovation.
The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps
(ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis
process. The user can define temporal, text mining and compound attributes. The
text mining attributes are used to analyze the unstructured text in documents,
the temporal attributes use these document's timestamps for analysis. The
compound attributes are XML rules based on text mining and temporal attributes.
The user can cluster objects with object-cluster rules and can chop the data in
pieces with segmentation rules. The artifacts are optimized for efficient data
analysis; object labels in the FCA lattice and ESOM map contain an URL on which
the user can click to open the selected document.
","Jonas Poelmans, Paul Elzinga, Alexey Neznanov, Stijn Viaene, Sergei O. Kuznetsov, Dmitry Ignatov, Guido Dedene","Mon, 13 Feb 2012 23:19:51 GMT (415kb)",Artificial Intelligence (cs.AI),Concept Relation Discovery and Innovation Enabling Technology (CORDIET)
"Text mining is becoming vital as Web 2.0 offers collaborative content
creation and sharing. Now Researchers have growing interest in text mining
methods for discovering knowledge. Text mining researchers come from variety of
areas like: Natural Language Processing, Computational Linguistic, Machine
Learning, and Statistics. A typical text mining application involves
preprocessing of text, stemming and lemmatization, tagging and annotation,
deriving knowledge patterns, evaluating and interpreting the results. There are
numerous approaches for performing text mining tasks, like: clustering,
categorization, sentimental analysis, and summarization. There is a growing
need to standardize the evaluation of these tasks. One major component of
establishing standardization is to provide standard datasets for these tasks.
Although there are various standard datasets available for traditional text
mining tasks, but there are very few and expensive datasets for blog-mining
task. Blogs, a new genre in web 2.0 is a digital diary of web user, which has
chronological entries and contains a lot of useful knowledge, thus offers a lot
of challenges and opportunities for text mining. In this paper, we report a new
indigenous dataset for Pakistani Political Blogosphere. The paper describes the
process of data collection, organization, and standardization. We have used
this dataset for carrying out various text mining tasks for blogosphere, like:
blog-search, political sentiments analysis and tracking, identification of
influential blogger, and clustering of the blog-posts. We wish to offer this
dataset free for others who aspire to pursue further in this domain.
","Mehwish Aziz, Muhammad Rafi","Tue, 10 Jan 2012 15:18:38 GMT (91kb)",Artificial Intelligence (cs.AI),Pbm: A new dataset for blog mining
"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model
for probabilistic topic modeling, which attracts worldwide interests and
touches on many important applications in text mining, computer vision and
computational biology. This paper introduces a topic modeling toolbox (TMBP)
based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by
MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing
topic modeling packages, the novelty of this toolbox lies in the BP algorithms
for learning LDA-based topic models. The current version includes BP algorithms
for latent Dirichlet allocation (LDA), author-topic models (ATM), relational
topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project
and more BP-based algorithms for various topic models will be added in the near
future. Interested users may also extend BP algorithms for learning more
complicated topic models. The source codes are freely available under the GNU
General Public Licence, Version 1.0 at this https URL
",Jia Zeng,"Wed, 4 Jan 2012 07:07:06 GMT (15kb) [v2] Thu, 5 Apr 2012 06:48:35 GMT (9kb)",Learning (cs.LG),A Topic Modeling Toolbox Using Belief Propagation
"Nonnegative matrix factorization has been widely applied in face recognition,
text mining, as well as spectral analysis. This paper proposes an alternating
proximal gradient method for solving this problem. With a uniformly positive
lower bound assumption on the iterates, any limit point can be proved to
satisfy the first-order optimality conditions. A Nesterov-type extrapolation
technique is then applied to accelerate the algorithm. Though this technique is
at first used for convex program, it turns out to work very well for the
non-convex nonnegative matrix factorization problem. Extensive numerical
experiments illustrate the efficiency of the alternating proximal gradient
method and the accleration technique. Especially for real data tests, the
accelerated method reveals high superiority to state-of-the-art algorithms in
speed with comparable solution qualities.
",Yangyang Xu,"Thu, 22 Dec 2011 18:22:59 GMT (362kb,D) [v2] Sat, 29 Dec 2012 17:43:05 GMT (0kb,I) [v3] Mon, 11 Feb 2013 17:54:57 GMT (0kb,I)",Information Theory (cs.IT),Alternating proximal gradient method for nonnegative matrix factorization
"Micro-blogging systems such as Twitter expose digital traces of social
discourse with an unprecedented degree of resolution of individual behaviors.
They offer an opportunity to investigate how a large-scale social system
responds to exogenous or endogenous stimuli, and to disentangle the temporal,
spatial and topical aspects of users' activity. Here we focus on spikes of
collective attention in Twitter, and specifically on peaks in the popularity of
hashtags. Users employ hashtags as a form of social annotation, to define a
shared context for a specific event, topic, or meme. We analyze a large-scale
record of Twitter activity and find that the evolution of hastag popularity
over time defines discrete classes of hashtags. We link these dynamical classes
to the events the hashtags represent and use text mining techniques to provide
a semantic characterization of the hastag classes. Moreover, we track the
propagation of hashtags in the Twitter social network and find that epidemic
spreading plays a minor role in hastag popularity, which is mostly driven by
exogenous factors.
","Janette Lehmann, Bruno Gon?alves, Jos? J. Ramasco, Ciro Cattuto","Tue, 8 Nov 2011 13:06:22 GMT (1388kb,D) [v2] Thu, 1 Mar 2012 17:45:52 GMT (1379kb,D)",Social and Information Networks (cs.SI),Dynamical Classes of Collective Attention in Twitter
"Recent studies have shown strong correlation between social networking data
and national influenza rates. We expanded upon this success to develop an
automated text mining system that classifies Twitter messages in real time into
six syndromic categories based on key terms from a public health ontology.
10-fold cross validation tests were used to compare Naive Bayes (NB) and
Support Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM
performed better than NB on 4 out of 6 syndromes. The best performing
classifiers showed moderately strong F1 scores: respiratory = 86.2 (NB);
gastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6
(SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1);
constitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9
(NB). The resulting classifiers were deployed together with an EARS C2
aberration detection algorithm in an experimental online system.
","Nigel Collier, Son Doan","Thu, 13 Oct 2011 23:42:32 GMT (108kb,D)",Computation and Language (cs.CL),Syndromic classification of Twitter messages
"Background: Accurate and timely detection of public health events of
international concern is necessary to help support risk assessment and response
and save lives. Novel event-based methods that use the World Wide Web as a
signal source offer potential to extend health surveillance into areas where
traditional indicator networks are lacking. In this paper we address the issue
of systematically evaluating online health news to support automatic alerting
using daily disease-country counts text mined from real world data using
BioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration
detection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance
against expert moderated ProMED-mail postings. Results: We report sensitivity,
specificity, positive predictive value (PPV), negative predictive value (NPV),
mean alerts/100 days and F1, at 95% confidence interval (CI) for 287
ProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period.
Results indicate that W2 had the best F1 with a slight benefit for day of week
effect over C2. In drill down analysis we indicate issues arising from the
granular choice of country-level modeling, sudden drops in reporting due to day
of week effects and reporting bias. Automatic alerting has been implemented in
BioCaster available from this http URL Conclusions: Online health news
alerts have the potential to enhance manual analytical methods by increasing
throughput, timeliness and detection rates. Systematic evaluation of health
news aberrations is necessary to push forward our understanding of the complex
relationship between news report volumes and case numbers and to select the
best performing features and algorithms.
",Nigel Collier,"Thu, 13 Oct 2011 23:22:43 GMT (1759kb)",Computation and Language (cs.CL),What's unusual in online disease outbreak news?
"Background: Online news reports are increasingly becoming a source for event
based early warning systems that detect natural disasters. Harnessing the
massive volume of information available from multilingual newswire presents as
many challenges as opportunities due to the patterns of reporting complex
spatiotemporal events. Results: In this article we study the problem of
utilising correlated event reports across languages. We track the evolution of
16 disease outbreaks using 5 temporal aberration detection algorithms on
text-mined events classified according to disease and outbreak country. Using
ProMED reports as a silver standard, comparative analysis of news data for 13
languages over a 129 day trial period showed improved sensitivity, F1 and
timeliness across most models using cross-lingual events. We report a detailed
case study analysis for Cholera in Angola 2010 which highlights the challenges
faced in correlating news events with the silver standard. Conclusions: The
results show that automated health surveillance using multilingual text mining
has the potential to turn low value news into high value alerts if informed
choices are used to govern the selection of models and data sources. An
implementation of the C2 alerting algorithm using multilingual news is
available at the BioCaster portal this http URL
",Nigel Collier,"Thu, 13 Oct 2011 23:05:48 GMT (944kb)",Computation and Language (cs.CL),Towards cross-lingual alerting for bursty epidemic events
"We present the alpha release of the VOGCLUSTERS web application, specialized
for data and text mining on globular clusters. It is one of the web2.0
technology based services of Data Mining & Exploration (DAME) Program, devoted
to mine and explore heterogeneous information related to globular clusters
data.
","Marco Castellani, Massimo Brescia, Ettore Mancini, Luca Pellecchia, Giuseppe Longo","Mon, 19 Sep 2011 17:33:39 GMT (232kb) [v2] Thu, 22 Sep 2011 10:57:57 GMT (232kb)",Instrumentation and Methods for Astrophysics (astro-ph.IM),VOGCLUSTERS: an example of DAME web application
"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model
for probabilistic topic modeling, which attracts worldwide interests and
touches on many important applications in text mining, computer vision and
computational biology. This paper represents LDA as a factor graph within the
Markov random field (MRF) framework, which enables the classic loopy belief
propagation (BP) algorithm for approximate inference and parameter estimation.
Although two commonly-used approximate inference methods, such as variational
Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in
learning LDA, the proposed BP is competitive in both speed and accuracy as
validated by encouraging experimental results on four large-scale document data
sets. Furthermore, the BP algorithm has the potential to become a generic
learning scheme for variants of LDA-based topic models. To this end, we show
how to learn two typical variants of LDA-based topic models, such as
author-topic models (ATM) and relational topic models (RTM), using BP based on
the factor graph representation.
","Jia Zeng, William K. Cheung, Jiming Liu","Thu, 15 Sep 2011 19:20:48 GMT (5383kb) [v2] Sun, 25 Sep 2011 21:17:41 GMT (5383kb) [v3] Mon, 3 Oct 2011 03:17:44 GMT (5978kb,S) [v4] Sat, 24 Mar 2012 12:47:02 GMT (5419kb)",Learning (cs.LG),Learning Topic Models by Belief Propagation
"VOSviewer is a computer program for creating, visualizing, and exploring
bibliometric maps of science. In this report, the new text mining functionality
of VOSviewer is presented. A number of examples are given of applications in
which VOSviewer is used for analyzing large amounts of text data.
","Nees Jan van Eck, Ludo Waltman","Fri, 9 Sep 2011 16:16:22 GMT (920kb)",Digital Libraries (cs.DL),Text mining and visualization using VOSviewer
"Information and communication technology has the capability to improve the
process by which governments involve citizens in formulating public policy and
public projects. Even though much of government regulations may now be in
digital form (and often available online), due to their complexity and
diversity, identifying the ones relevant to a particular context is a
non-trivial task. Similarly, with the advent of a number of electronic online
forums, social networking sites and blogs, the opportunity of gathering
citizens' petitions and stakeholders' views on government policy and proposals
has increased greatly, but the volume and the complexity of analyzing
unstructured data makes this difficult. On the other hand, text mining has come
a long way from simple keyword search, and matured into a discipline capable of
dealing with much more complex tasks. In this paper we discuss how text-mining
techniques can help in retrieval of information and relationships from textual
data sources, thereby assisting policy makers in discovering associations
between policies and citizens' opinions expressed in electronic public forums
and blogs etc. We also present here, an integrated text mining based
architecture for e-governance decision support along with a discussion on the
Indian scenario.
","G.Koteswara Rao, Shubhamoy Dey","Wed, 31 Aug 2011 11:48:57 GMT (529kb)",Databases (cs.DB),Decision Support for e-Governance: A Text Mining Approach
"Based on the concept of annotation-based agents, this report introduces tools
and a formal notation for defining and running text mining experiments using a
statically typed domain-specific language embedded in Scala. Using machine
learning for classification as an example, the framework is used to develop and
document text mining experiments, and to show how the concept of generic,
typesafe annotation corresponds to a general information model that goes beyond
text processing.
",Fabian Steeg,"Thu, 28 Jul 2011 17:46:20 GMT (419kb,D)",Programming Languages (cs.PL),Typesafe Modeling in Text Mining
"Nonnegative matrix factorization (NMF) is a data analysis technique used in a
great variety of applications such as text mining, image processing,
hyperspectral data analysis, computational biology, and clustering. In this
paper, we consider two well-known algorithms designed to solve NMF problems,
namely the multiplicative updates of Lee and Seung and the hierarchical
alternating least squares of Cichocki et al. We propose a simple way to
significantly accelerate these schemes, based on a careful analysis of the
computational cost needed at each iteration, while preserving their convergence
properties. This acceleration technique can also be applied to other
algorithms, which we illustrate on the projected gradient method of Lin. The
efficiency of the accelerated algorithms is empirically demonstrated on image
and text datasets, and compares favorably with a state-of-the-art alternating
nonnegative least squares algorithm.
","Nicolas Gillis, Fran?ois Glineur","Tue, 26 Jul 2011 12:26:07 GMT (306kb) [v2] Thu, 6 Oct 2011 13:16:20 GMT (787kb)",Optimization and Control (math.OC),Accelerated Multiplicative Updates and Hierarchical ALS Algorithms for Nonnegative Matrix Factorization
"Model-based language specification has applications in the implementation of
language processors, the design of domain-specific languages, model-driven
software development, data integration, text mining, natural language
processing, and corpus-based induction of models. Model-based language
specification decouples language design from language processing and, unlike
traditional grammar-driven approaches, which constrain language designers to
specific kinds of grammars, it needs general parser generators able to deal
with ambiguities. In this paper, we propose Fence, an efficient bottom-up
parsing algorithm with lexical and syntactic ambiguity support that enables the
use of model-based language specification in practice.
","Luis Quesada, Fernando Berzal, Francisco J. Cortijo","Sat, 23 Jul 2011 12:56:02 GMT (308kb) [v2] Fri, 7 Oct 2011 09:50:12 GMT (298kb)",Computation and Language (cs.CL),Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification
"Markov Logic Networks (MLNs) have emerged as a powerful framework that
combines statistical and logical reasoning; they have been applied to many data
intensive problems including information extraction, entity resolution, and
text mining. Current implementations of MLNs do not scale to large real-world
data sets, which is preventing their wide-spread adoption. We present Tuffy
that achieves scalability via three novel contributions: (1) a bottom-up
approach to grounding that allows us to leverage the full power of the
relational optimizer, (2) a novel hybrid architecture that allows us to perform
AI-style local search efficiently using an RDBMS, and (3) a theoretical insight
that shows when one can (exponentially) improve the efficiency of stochastic
local search. We leverage (3) to build novel partitioning, loading, and
parallel algorithms. We show that our approach outperforms state-of-the-art
implementations in both quality and speed on several publicly available
datasets.
","Feng Niu (University of Wisconsin-Madison), Christopher R? (University of Wisconsin-Madison), AnHai Doan (University of Wisconsin-Madison), Jude Shavlik (University of Wisconsin-Madison)","Sat, 16 Apr 2011 08:52:25 GMT (1437kb)",Databases (cs.DB),Tuffy: Scaling up Statistical Inference in Markov Logic Networks using an RDBMS
"Unstructured information comprises a valuable source of data in clinical
records. For text mining in clinical records, concept extraction is the first
step in finding assertions and relationships. This study presents a system
developed for the annotation of medical concepts, including medical problems,
tests, and treatments, mentioned in clinical records. The system combines six
publicly available named entity recognition system into one framework, and uses
a simple voting scheme that allows to tune precision and recall of the system
to specific needs. The system provides both a web service interface and a UIMA
interface which can be easily used by other systems. The system was tested in
the fourth i2b2 challenge and achieved an F-score of 82.1% for the concept
exact match task, a score which is among the top-ranking systems. To our
knowledge, this is the first publicly available clinical record concept
annotation system.
","Ning Kang, Rogier Barendse, Zubair Afzal, Bharat Singh, Martijn J. Schuemie, Erik M. van Mulligen, Jan A. Kors","Wed, 8 Dec 2010 00:56:45 GMT (297kb)",Information Retrieval (cs.IR),A Concept Annotation System for Clinical Records
"Integration of the scientific literature into a biomedical research
infrastructure requires the processing of the literature, identification of the
contained named entities (NEs) and concepts, and to represent the content in a
standardised way. The CALBC project partners (PPs) have produced a large-scale
annotated biomedical corpus with four different semantic groups through the
harmonisation of annotations from automatic text mining solutions (Silver
Standard Corpus, SSC). The four semantic groups were chemical entities and
drugs (CHED), genes and proteins (PRGE), diseases and disorders (DISO) and
species (SPE). The content of the SSC has been fully integrated into RDF Triple
Store (4,568,678 triples) and has been aligned with content from the GeneAtlas
(182,840 triples), UniProtKb (12,552,239 triples for human) and the lexical
resource LexEBI (BioLexicon). RDF Triple Store enables querying the scientific
literature and bioinformatics resources at the same time for evidence of
genetic causes, such as drug targets and disease involvement.
","Samuel Croset, Christoph Grabm?ller, Chen Li, Silvestras Kavaliauskas, Dietrich Rebholz-Schuhmann","Wed, 8 Dec 2010 00:19:06 GMT (585kb)",Digital Libraries (cs.DL),The CALBC RDF Triple Store: retrieval over large literature content
"In this paper we dealt with the comparison and linking between lexical
resources with domain knowledge provided by ontologies. It is one of the issues
for the combination of the Semantic Web Ontologies and Text Mining. We
investigated the relations between the linguistics oriented and domain-specific
semantics, by associating the GO biological process concepts to the FrameNet
semantic frames. The result shows the gaps between the linguistics-oriented and
domain-specific semantics on the classification of events and the grouping of
target words. The result provides valuable information for the improvement of
domain ontologies supporting for text mining systems. And also, it will result
in benefits to language understanding technology.
",He Tan,"Tue, 7 Dec 2010 23:03:42 GMT (30kb)",Artificial Intelligence (cs.AI),A study on the relation between linguistics-oriented and domain-specific semantics
"Nonnegative Matrix Factorization (NMF) is the problem of approximating a
nonnegative matrix with the product of two low-rank nonnegative matrices and
has been shown to be particularly useful in many applications, e.g., in text
mining, image processing, computational biology, etc. In this paper, we explain
how algorithms for NMF can be embedded into the framework of multilevel methods
in order to accelerate their convergence. This technique can be applied in
situations where data admit a good approximate representation in a lower
dimensional space through linear transformations preserving nonnegativity. A
simple multilevel strategy is described and is experimentally shown to speed up
significantly three popular NMF algorithms (alternating nonnegative least
squares, multiplicative updates and hierarchical alternating least squares) on
several standard image datasets.
","Nicolas Gillis, Fran?ois Glineur","Sat, 4 Sep 2010 22:55:34 GMT (575kb) [v2] Sun, 12 Sep 2010 16:47:01 GMT (554kb) [v3] Tue, 4 Oct 2011 00:02:21 GMT (569kb)",Optimization and Control (math.OC),A Multilevel Approach For Nonnegative Matrix Factorization
"With the advancement of technology and reduced storage costs, individuals and
organizations are tending towards the usage of electronic media for storing
textual information and documents. It is time consuming for readers to retrieve
relevant information from unstructured document collection. It is easier and
less time consuming to find documents from a large collection when the
collection is ordered or classified by group or category. The problem of
finding best such grouping is still there. This paper discusses the
implementation of k-Means clustering algorithm for clustering unstructured text
documents that we implemented, beginning with the representation of
unstructured text and reaching the resulting set of clusters. Based on the
analysis of resulting clusters for a sample set of documents, we have also
proposed a technique to represent documents that can further improve the
clustering result.
","Yasir Safeer (1), Atika Mustafa (1), Anis Noor Ali (1) ((1) FAST - National University of Computer and Emerging Sciences)","Sun, 25 Jul 2010 14:38:32 GMT (1045kb)",Information Retrieval (cs.IR),Clustering Unstructured Data (Flat Files) - An Implementation in Text Mining Tool
"Keyphrases provide a simple way of describing a document, giving the reader
some clues about its contents. Keyphrases can be useful in a various
applications such as retrieval engines, browsing interfaces, thesaurus
construction, text mining etc.. There are also other tasks for which keyphrases
are useful, as we discuss in this paper. This paper describes a neural network
based approach to keyphrase extraction from scientific articles. Our results
show that the proposed method performs better than some state-of-the art
keyphrase extraction approaches.
","Kamal Sarkar, Mita Nasipuri, Suranjan Ghose","Mon, 19 Apr 2010 18:24:02 GMT (110kb)",Information Retrieval (cs.IR),A New Approach to Keyphrase Extraction Using Neural Networks
"Computer System Administration and Network Administration are few such areas
where Practical Extraction Reporting Language (Perl) has robust utilization
these days apart from Bioinformatics. The key role of a System/Network
Administrator is to monitor log files. Log file are updated every day. To scan
the summary of large log files and to quickly determine if there is anything
wrong with the server or network we develop a Firewall Log Status Reporter
(SRr). SRr helps to generate the reports based on the parameters of interest.
SRr provides the facility to admin to generate the individual firewall report
or all reports in one go. By scrutinizing the results of the reports admin can
trace how many times a particular request has been made from which source to
which destination and can track the errors easily. Perl scripts can be seen as
the UNIX script replacement in future arena and SRr is one development with the
same hope that we can believe in. SRr is a generalized and customizable utility
completely written in Perl and may be used for text mining and data mining
application in Bioinformatics research and development too.
","Sugam Sharma (1), Hari Cohly (2), Tzusheng Pei (2),  ((1)Iowa State University, USA, (2) Jackson State University, USA)","Mon, 5 Apr 2010 09:52:05 GMT (292kb)",Software Engineering (cs.SE),On Generation of Firewall Log Status Reporter (SRr) Using Perl
"The complexity of sentences characteristic to biomedical articles poses a
challenge to natural language parsers, which are typically trained on
large-scale corpora of non-technical text. We propose a text simplification
process, bioSimplify, that seeks to reduce the complexity of sentences in
biomedical abstracts in order to improve the performance of syntactic parsers
on the processed sentences. Syntactic parsing is typically one of the first
steps in a text mining pipeline. Thus, any improvement in performance would
have a ripple effect over all processing steps. We evaluated our method using a
corpus of biomedical sentences annotated with syntactic links. Our empirical
results show an improvement of 2.90% for the Charniak-McClosky parser and of
4.23% for the Link Grammar parser when processing simplified sentences rather
than the original sentences in the corpus.
","Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg, Chitta Baral, Graciela Gonzalez","Sun, 24 Jan 2010 20:35:29 GMT (58kb)",Computation and Language (cs.CL),Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text
"In Bioinformatics, text mining and text data mining sometimes interchangeably
used is a process to derive high-quality information from text. Perl Status
Reporter (SRr) is a data fetching tool from a flat text file and in this
research paper we illustrate the use of SRr in text or data mining. SRr needs a
flat text input file where the mining process to be performed. SRr reads input
file and derives the high quality information from it. Typically text mining
tasks are text categorization, text clustering, concept and entity extraction,
and document summarization. SRr can be utilized for any of these tasks with
little or none customizing efforts. In our implementation we perform text
categorization mining operation on input file. The input file has two
parameters of interest (firstKey and secondKey). The composition of these two
parameters describes the uniqueness of entries in that file in the similar
manner as done by composite key in database. SRr reads the input file line by
line and extracts the parameters of interest and form a composite key by
joining them together. It subsequently generates an output file consisting of
the name as firstKey secondKey. SRr reads the input file and tracks the
composite key. It further stores all that data lines, having the same composite
key, in output file generated by SRr based on that composite key.
","Sugam Sharma, Tzusheng Pei, Hari Cohly","Tue, 19 Jan 2010 12:16:55 GMT (756kb) [v2] Thu, 21 Jan 2010 21:21:22 GMT (749kb)",Information Retrieval (cs.IR),On Utilization and Importance of Perl Status Reporter (SRr) in Text Mining
"Metric and kernel learning are important in several machine learning
applications. However, most existing metric learning algorithms are limited to
learning metrics over low-dimensional data, while existing kernel learning
algorithms are often limited to the transductive setting and do not generalize
to new data points. In this paper, we study metric learning as a problem of
learning a linear transformation of the input data. We show that for
high-dimensional data, a particular framework for learning a linear
transformation of the data based on the LogDet divergence can be efficiently
kernelized to learn a metric (or equivalently, a kernel function) over an
arbitrarily high dimensional space. We further demonstrate that a wide class of
convex loss functions for learning linear transformations can similarly be
kernelized, thereby considerably expanding the potential applications of metric
learning. We demonstrate our learning approach by applying it to large-scale
real world problems in computer vision and text mining.
","Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon","Fri, 30 Oct 2009 18:19:03 GMT (75kb)",Learning (cs.LG),Metric and Kernel Learning using a Linear Transformation
"Nonnegative Matrix Factorization consists in (approximately) factorizing a
nonnegative data matrix by the product of two low-rank nonnegative matrices. It
has been successfully applied as a data analysis technique in numerous domains,
e.g., text mining, image processing, microarray data analysis, collaborative
filtering, etc.
We introduce a novel approach to solve NMF problems, based on the use of an
underapproximation technique, and show its effectiveness to obtain sparse
solutions. This approach, based on Lagrangian relaxation, allows the resolution
of NMF problems in a recursive fashion. We also prove that the
underapproximation problem is NP-hard for any fixed factorization rank, using a
reduction of the maximum edge biclique problem in bipartite graphs.
We test two variants of our underapproximation approach on several standard
image datasets and show that they provide sparse part-based representations
with low reconstruction error. Our results are comparable and sometimes
superior to those obtained by two standard Sparse Nonnegative Matrix
Factorization techniques.
","Nicolas Gillis, Fran?ois Glineur","Fri, 6 Feb 2009 11:05:23 GMT (457kb,D) [v2] Sat, 24 Oct 2009 17:02:58 GMT (511kb,D)",Optimization and Control (math.OC),Using Underapproximations for Sparse Nonnegative Matrix Factorization
"This paper proposes a novel solution to spam detection inspired by a model of
the adaptive immune system known as the crossregulation model. We report on the
testing of a preliminary algorithm on six e-mail corpora. We also compare our
results statically and dynamically with those obtained by the Naive Bayes
classifier and another binary classification method we developed previously for
biomedical text-mining applications. We show that the cross-regulation model is
competitive against those and thus promising as a bio-inspired algorithm for
spam detection in particular, and binary classification in general.
","Alaa Abi-Haidar, Luis M. Rocha","Thu, 4 Dec 2008 20:40:32 GMT (298kb,D)",Artificial Intelligence (cs.AI),Adaptive Spam Detection Inspired by a Cross-Regulation Model of Immune Dynamics: A Study of Concept Drift
"The exponential increase in publication rate of new articles is limiting
access of researchers to relevant literature. This has prompted the use of text
mining tools to extract key biological information. Previous studies have
reported extensive modification of existing generic text processors to process
biological text. However, this requirement for modification had not been
examined. In this study, we have constructed Muscorian, using MontyLingua, a
generic text processor. It uses a two-layered generalization-specialization
paradigm previously proposed where text was generically processed to a suitable
intermediate format before domain-specific data extraction techniques are
applied at the specialization layer. Evaluation using a corpus and experts
indicated 86-90% precision and approximately 30% recall in extracting
protein-protein interactions, which was comparable to previous studies using
either specialized biological text processing tools or modified existing tools.
Our study had also demonstrated the flexibility of the two-layered
generalization-specialization paradigm by using the same generalization layer
for two specialized information extraction tasks.
","Maurice HT Ling, Christophe Lefevre, Kevin R. Nicholas, Feng Lin","Mon, 6 Aug 2007 01:22:46 GMT (295kb)",Information Retrieval (cs.IR),Reconstruction of Protein-Protein Interaction Pathways by Mining Subject-Verb-Objects Intermediates
"Open Source Software (OSS) development challenges traditional software
engineering practices. In particular, OSS projects are managed by a large
number of volunteers, working freely on the tasks they choose to undertake. OSS
projects also rarely rely on explicit system-level design, or on project plans
or schedules. Moreover, OSS developers work in arbitrary locations and
collaborate almost exclusively over the Internet, using simple tools such as
email and software code tracking databases (e.g. CVS). All the characteristics
above make OSS development akin to weaving a tapestry of heterogeneous
components. The OSS design process relies on various types of actors: people
with prescribed roles, but also elements coming from a variety of information
spaces (such as email and software code). The objective of our research is to
understand the specific hybrid weaving accomplished by the actors of this
distributed, collective design process. This, in turn, challenges traditional
methodologies used to understand distributed software engineering: OSS
development is simply too ""fibrous"" to lend itself well to analysis under a
single methodological lens. In this paper, we describe the methodological
framework we articulated to analyze collaborative design in the Open Source
world. Our framework focuses on the links between the heterogeneous components
of a project's hybrid network. We combine ethnography, text mining, and
socio-technical network analysis and visualization to understand OSS
development in its totality. This way, we are able to simultaneously consider
the social, technical, and cognitive aspects of OSS development. We describe
our methodology in detail, and discuss its implications for future research on
distributed collective practices.
","Warren Sack, Fran?oise D?tienne (INRIA Rocquencourt), Nicholas Ducheneaut, Jean-Marie Burkhardt (LEI), Dilan Mahendran, Flore Barcellini (INRIA Rocquencourt, EIFFEL)","Fri, 2 Mar 2007 13:51:29 GMT (410kb)",Human-Computer Interaction (cs.HC),A Methodological Framework for Socio-Cognitive Analyses of Collaborative Design of Open Source Software
"The automatic extraction of acronyms and their meaning from corpora is an
important sub-task of text mining. It can be seen as a special case of string
alignment, where a text chunk is aligned with an acronym. Alternative
alignments have different cost, and ideally the least costly one should give
the correct meaning of the acronym. We show how this approach can be
implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which
reads a text chunk on tape 1 and an acronym on tape 2, and generates all
alternative alignments on tape 3. The 3-WFSM can be automatically generated
from a simple regular expression. No additional algorithms are required at any
stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the
best analysis of an acronym in a few milliseconds.
",Andr? Kempe,"Wed, 6 Dec 2006 10:13:12 GMT (8kb)",Computation and Language (cs.CL),Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted Finite-State Machines
"We explore a matrix-space model, that is a natural extension to the vector
space model for Information Retrieval. Each document can be represented by a
matrix that is based on document extracts (e.g. sentences, paragraphs,
sections). We focus on the performance of this model for the specific case in
which documents are originally represented as term-by-sentence matrices. We use
the singular value decomposition to approximate the term-by-sentence matrices
and assemble these results to form the pseudo-``term-document'' matrix that
forms the basis of a text mining method alternative to traditional VSM and LSI.
We investigate the singular values of this matrix and provide experimental
evidence suggesting that the method can be particularly effective in terms of
accuracy for text collections with multi-topic documents, such as web pages
with news.
","Ioannis Antonellis, Efstratios Gallopoulos","Tue, 21 Feb 2006 16:14:16 GMT (193kb)",Information Retrieval (cs.IR),Exploring term-document matrices from matrix models in text mining
"A key data preparation step in Text Mining, Term Extraction selects the
terms, or collocation of words, attached to specific concepts. In this paper,
the task of extracting relevant collocations is achieved through a supervised
learning algorithm, exploiting a few collocations manually labelled as
relevant/irrelevant. The candidate terms are described along 13 standard
statistical criteria measures. From these examples, an evolutionary learning
algorithm termed Roger, based on the optimization of the Area under the ROC
curve criterion, extracts an order on the candidate terms. The robustness of
the approach is demonstrated on two real-world domain applications, considering
different domains (biology and human resources) and different languages
(English and French).
","J?r?me Az? (LRI), Mathieu Roche (LRI), Yves Kodratoff (LRI), Mich?le Sebag (LRI)","Tue, 13 Dec 2005 13:25:57 GMT (34kb)",Learning (cs.LG),Preference Learning in Terminology Extraction: A ROC-based approach
"Transitive text mining - also named Swanson Linking (SL) after its primary
and principal researcher - tries to establish meaningful links between
literature sets which are virtually disjoint in the sense that each does not
mention the main concept of the other. If successful, SL may give rise to the
development of new hypotheses. In this communication we describe our approach
to transitive text mining which employs co-occurrence analysis of the medical
subject headings (MeSH), the descriptors assigned to papers indexed in PubMed.
In addition, we will outline the current state of our web-based information
system which will enable our users to perform literature-driven hypothesis
building on their own.
","Johannes Stegmann, Guenter Grohmann (Charite, Berlin)","Wed, 7 Sep 2005 12:16:22 GMT (15kb)",Information Retrieval (cs.IR),Transitive Text Mining for Information Extraction and Hypothesis Generation
"Knowledge discovery is defined as non-trivial extraction of implicit,
previously unknown and potentially useful information from given data.
Knowledge extraction from web documents deals with unstructured, free-format
documents whose number is enormous and rapidly growing. The artificial neural
networks are well suitable to solve a problem of knowledge discovery from web
documents because trained networks are able more accurately and easily to
classify the learning and testing examples those represent the text mining
domain. However, the neural networks that consist of large number of weighted
connections and activation units often generate the incomprehensible and
hard-to-understand models of text classification. This problem may be also
addressed to most powerful recurrent neural networks that employ the feedback
links from hidden or output units to their input units. Due to feedback links,
recurrent neural networks are able take into account of a context in document.
To be useful for data mining, self-organizing neural network techniques of
knowledge extraction have been explored and developed. Self-organization
principles were used to create an adequate neural-network structure and reduce
a dimensionality of features used to describe text documents. The use of these
principles seems interesting because ones are able to reduce a neural-network
redundancy and considerably facilitate the knowledge representation.
",Vitaly Schetinin,"Wed, 13 Apr 2005 13:40:38 GMT (64kb)",Neural and Evolutionary Computing (cs.NE),Learning from Web: Review of Approaches
"Several Networks of Excellence have been set up in the framework of the
European FP5 research program. Among these Networks of Excellence, the NEMIS
project focuses on the field of Text Mining.
Within this field, document processing and visualization was identified as
one of the key topics and the WG1 working group was created in the NEMIS
project, to carry out a detailed survey of techniques associated with the text
mining process and to identify the relevant research topics in related research
areas.
In this document we present the results of this comprehensive survey. The
report includes a description of the current state-of-the-art and practice, a
roadmap for follow-up research in the identified areas, and recommendations for
anticipated technological development in the domain of text mining.
","Martin Rajman, Martin Vesely, Pierre Andrews","Wed, 29 Dec 2004 15:19:03 GMT (666kb)",Computation and Language (cs.CL),"State of the Art, Evaluation and Recommendations regarding ""Document Processing and Visualization Techniques"""
"MOTIVATION: The biological literature is a major repository of knowledge.
Many biological databases draw much of their content from a careful curation of
this literature. However, as the volume of literature increases, the burden of
curation increases. Text mining may provide useful tools to assist in the
curation process. To date, the lack of standards has made it impossible to
determine whether text mining techniques are sufficiently mature to be useful.
RESULTS: We report on a Challenge Evaluation task that we created for the
Knowledge Discovery and Data Mining (KDD) Challenge Cup. We provided a training
corpus of 862 articles consisting of journal articles curated in FlyBase, along
with the associated lists of genes and gene products, as well as the relevant
data fields from FlyBase. For the test, we provided a corpus of 213 new
(`blind') articles; the 18 participating groups provided systems that flagged
articles for curation, based on whether the article contained experimental
evidence for gene expression products. We report on the the evaluation results
and describe the techniques used by the top performing groups.
CONTACT: asy@mitre.org
KEYWORDS: text mining, evaluation, curation, genomics, data management
","Alexander S. Yeh, Lynette Hirschman, Alexander A. Morgan","Wed, 20 Aug 2003 18:40:39 GMT (57kb)",Computation and Language (cs.CL),Evaluation of text data mining for database curation: lessons learned from the KDD Challenge Cup
"In this paper we present a phenomenological approach to describe a complex
system: scientific research impact through Citation Mining. The novel concept
of Citation Mining, a combination of citation bibliometrics and text mining, is
used for the phenomenological description. Citation Mining starts with a group
of core papers whose impact is to be examined, retrieves the papers that cite
these core papers, and then analyzes the technical infrastructure (authors,
jorunals, institutions) of the citing papers as well as their thematic
characteristics.
","J.A del Rio, R.N. Kostoff, E.O. Garcia, A.M. Ramirez, J. A. Humenik","Mon, 17 Dec 2001 05:46:54 GMT (264kb)",Physics and Society (physics.soc-ph),Phenomenological approach to profile impact of scientific research: Citation Mining
"Text mining is about looking for patterns in natural language text, and may
be defined as the process of analyzing text to extract information from it for
particular purposes. In previous work, we claimed that compression is a key
technology for text mining, and backed this up with a study that showed how
particular kinds of lexical tokens---names, dates, locations, etc.---can be
identified and located in running text, using compression models to provide the
leverage necessary to distinguish different token types (Witten et al., 1999)
","Stuart Yeates, David Bainbridge, Ian H. Witten","Tue, 4 Jul 2000 02:02:01 GMT (34kb)",Digital Libraries (cs.DL),Using compression to identify acronyms in text
